{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Clustering Internals: Architecture and Data Structures\n",
    "\n",
    "A deep dive into how time series clustering works under the hood.\n",
    "\n",
    "This notebook covers:\n",
    "\n",
    "- **Module overview**: The `flixopt.aggregation` module and its classes\n",
    "- **Data flow**: From `cluster()` through optimization to `expand_solution()`\n",
    "- **Core classes**: `ClusterStructure`, `ClusterResult`, `ClusterInfo`\n",
    "- **Cluster weights**: How operational costs are scaled correctly\n",
    "- **Storage linking**: Inter-cluster constraints for realistic storage behavior\n",
    "- **Multi-dimensional support**: Handling periods and scenarios\n",
    "\n",
    "!!! note \"Prerequisites\"\n",
    "    This notebook assumes familiarity with [08c-clustering](08c-clustering.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import flixopt as fx\n",
    "\n",
    "fx.CONFIG.notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the district heating system\n",
    "data_file = Path('data/district_heating_system.nc4')\n",
    "if not data_file.exists():\n",
    "    from data.generate_example_systems import create_district_heating_system\n",
    "\n",
    "    fs = create_district_heating_system()\n",
    "    fs.to_netcdf(data_file)\n",
    "\n",
    "flow_system = fx.FlowSystem.from_netcdf(data_file)\n",
    "print(f'Loaded: {len(flow_system.timesteps)} timesteps ({len(flow_system.timesteps) / 96:.0f} days)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Module Architecture Overview\n",
    "\n",
    "The clustering functionality lives in `flixopt.aggregation` with this structure:\n",
    "\n",
    "```\n",
    "flixopt.aggregation/\n",
    "├── base.py           # Core dataclasses: ClusterStructure, ClusterResult, ClusterInfo\n",
    "├── storage_linking.py # InterClusterLinking for storage constraints\n",
    "└── __init__.py       # Public exports\n",
    "```\n",
    "\n",
    "### Key Classes\n",
    "\n",
    "| Class | Purpose |\n",
    "|-------|--------|\n",
    "| `ClusterStructure` | Hierarchical structure: which original periods map to which clusters |\n",
    "| `ClusterResult` | Universal container: timestep mapping, weights, aggregated data |\n",
    "| `ClusterInfo` | Stored on FlowSystem after clustering; enables `expand_solution()` |\n",
    "| `InterClusterLinking` | Adds storage SOC constraints across the original time horizon |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the aggregation module to explore its contents\n",
    "from flixopt import aggregation\n",
    "\n",
    "print('Available in flixopt.aggregation:')\n",
    "print([name for name in dir(aggregation) if not name.startswith('_')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2. Data Flow: From `cluster()` to `expand_solution()`\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│  flow_system.transform.cluster(n_clusters=8, ...)               │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│  1. Extract time series data from FlowSystem                    │\n",
    "│  2. Call tsam for clustering                                    │\n",
    "│  3. Build ClusterStructure (cluster_order, occurrences)         │\n",
    "│  4. Build ClusterResult (timestep_mapping, weights)             │\n",
    "│  5. Create reduced FlowSystem with representative timesteps     │\n",
    "│  6. Store ClusterInfo on reduced_fs._cluster_info               │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│  reduced_fs.optimize(solver)                                    │\n",
    "│  └─ InterClusterLinking adds storage constraints if enabled     │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│  reduced_fs.transform.expand_solution()                         │\n",
    "│  └─ Uses stored timestep_mapping to expand back to full time    │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clustered system\n",
    "fs_clustered = flow_system.transform.cluster(\n",
    "    n_clusters=8,\n",
    "    cluster_duration='1D',\n",
    "    time_series_for_high_peaks=['HeatDemand(Q_th)|fixed_relative_profile'],\n",
    ")\n",
    "\n",
    "print(f'Original timesteps:  {len(flow_system.timesteps)}')\n",
    "print(f'Clustered timesteps: {len(fs_clustered.timesteps)}')\n",
    "print(f'Reduction: {len(flow_system.timesteps) / len(fs_clustered.timesteps):.1f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 3. The `ClusterInfo` Structure\n",
    "\n",
    "After clustering, metadata is stored in `fs._cluster_info`. This enables:\n",
    "- Expanding solutions back to full resolution\n",
    "- Understanding which original days map to which clusters\n",
    "- Correct weighting in the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = fs_clustered._cluster_info\n",
    "\n",
    "print('ClusterInfo attributes:')\n",
    "print(f'  backend_name:                 {info.backend_name}')\n",
    "print(f'  storage_inter_cluster_linking: {info.storage_inter_cluster_linking}')\n",
    "print(f'  storage_cyclic:               {info.storage_cyclic}')\n",
    "print(f'  original_flow_system:         {type(info.original_flow_system).__name__}')\n",
    "print(f'  result:                       {type(info.result).__name__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. The `ClusterStructure`: Hierarchical Mapping\n",
    "\n",
    "The `ClusterStructure` captures which original periods (days) belong to which clusters:\n",
    "\n",
    "- **`cluster_order`**: Array mapping each original period index to its cluster ID\n",
    "- **`cluster_occurrences`**: How many original periods each cluster represents\n",
    "- **`n_clusters`**: Number of representative clusters\n",
    "- **`timesteps_per_cluster`**: Timesteps in each cluster (e.g., 96 for daily with 15-min resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = info.result.cluster_structure\n",
    "\n",
    "print('ClusterStructure:')\n",
    "print(f'  n_clusters:           {cs.n_clusters}')\n",
    "print(f'  timesteps_per_cluster: {cs.timesteps_per_cluster}')\n",
    "print(f'  n_original_periods:   {cs.n_original_periods}')\n",
    "print(f'  cluster_order dims:   {cs.cluster_order.dims}')\n",
    "print(f'  cluster_order shape:  {cs.cluster_order.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_order shows which cluster each original day belongs to\n",
    "cluster_order = cs.cluster_order.values\n",
    "\n",
    "print('Cluster assignments (first 14 days):')\n",
    "for day in range(min(14, len(cluster_order))):\n",
    "    print(f'  Day {day + 1:2d} → Cluster {cluster_order[day]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_occurrences shows how many original days each cluster represents\n",
    "print('Cluster occurrences (days per cluster):')\n",
    "for cluster_id in range(cs.n_clusters):\n",
    "    count = int(cs.cluster_occurrences.sel(cluster=cluster_id).values)\n",
    "    print(f'  Cluster {cluster_id}: {count} day(s)')\n",
    "\n",
    "print(f'\\nTotal: {int(cs.cluster_occurrences.sum().values)} days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster assignment\n",
    "days_df = pd.DataFrame(\n",
    "    {\n",
    "        'Day': range(1, cs.n_original_periods + 1),\n",
    "        'Cluster': cluster_order,\n",
    "    }\n",
    ")\n",
    "\n",
    "fig = px.bar(\n",
    "    days_df,\n",
    "    x='Day',\n",
    "    y=[1] * len(days_df),\n",
    "    color='Cluster',\n",
    "    color_continuous_scale='Viridis',\n",
    "    title='Cluster Assignment by Day',\n",
    ")\n",
    "fig.update_layout(height=250, yaxis_visible=False, coloraxis_colorbar_title='Cluster')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 5. The `ClusterResult`: Timestep Mapping and Weights\n",
    "\n",
    "The `ClusterResult` contains:\n",
    "\n",
    "- **`timestep_mapping`**: Maps each original timestep to its representative timestep index\n",
    "- **`representative_weights`**: Weight for each representative timestep (used as `cluster_weight`)\n",
    "- **`cluster_structure`**: Reference to the hierarchical structure\n",
    "- **`original_data`**: The time series data used for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = info.result\n",
    "\n",
    "print('ClusterResult:')\n",
    "print(f'  n_representatives:        {result.n_representatives}')\n",
    "print(f'  timestep_mapping dims:    {result.timestep_mapping.dims}')\n",
    "print(f'  timestep_mapping shape:   {result.timestep_mapping.shape}')\n",
    "print(f'  representative_weights:   {result.representative_weights.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The timestep_mapping shows which representative timestep each original timestep maps to\n",
    "mapping = result.timestep_mapping.values\n",
    "\n",
    "print('Timestep mapping (first 10 original timesteps):')\n",
    "for t in range(10):\n",
    "    print(f'  Original t={t} → Representative t={mapping[t]}')\n",
    "\n",
    "print(f'\\n... (total {len(mapping)} mappings)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 6. Cluster Weights: Scaling Operational Costs\n",
    "\n",
    "When optimizing over typical periods, operational costs must be **scaled** to represent the full time horizon.\n",
    "\n",
    "### The Weight Formula\n",
    "\n",
    "$$\\text{Objective} = \\sum_{t \\in \\text{typical}} w_t \\cdot c_t$$\n",
    "\n",
    "Where:\n",
    "- $w_t$ = cluster weight for timestep $t$ (number of original days this cluster represents)\n",
    "- $c_t$ = operational cost at timestep $t$\n",
    "\n",
    "### Weight Conservation\n",
    "\n",
    "$$\\sum_{t \\in \\text{typical}} w_t = |\\text{original timesteps}|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cluster_weight is stored on the FlowSystem\n",
    "print('cluster_weight on FlowSystem:')\n",
    "print(f'  Shape: {fs_clustered.cluster_weight.shape}')\n",
    "print(f'  Sum:   {fs_clustered.cluster_weight.sum().item():.0f}')\n",
    "print(f'  Expected (original timesteps): {len(flow_system.timesteps)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize weights across timesteps\n",
    "weights = fs_clustered.cluster_weight.values\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(weights))),\n",
    "        y=weights,\n",
    "        mode='lines',\n",
    "        name='Cluster Weight',\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add vertical lines at cluster boundaries\n",
    "for i in range(1, cs.n_clusters):\n",
    "    fig.add_vline(x=i * cs.timesteps_per_cluster, line_dash='dash', line_color='gray', opacity=0.5)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=300,\n",
    "    title='Cluster Weight per Timestep',\n",
    "    xaxis_title='Timestep Index',\n",
    "    yaxis_title='Weight',\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 7. Storage Inter-Cluster Linking\n",
    "\n",
    "Storage behavior requires special handling in clustering. The `InterClusterLinking` class:\n",
    "\n",
    "1. Creates **SOC_boundary** variables for each original period boundary\n",
    "2. Computes **delta_SOC** for each representative period (change in SOC)\n",
    "3. Links them: `SOC_boundary[d+1] = SOC_boundary[d] + delta_SOC[cluster_order[d]]`\n",
    "4. Optionally enforces cyclic constraint: `SOC_boundary[0] = SOC_boundary[end]`\n",
    "\n",
    "This tracks storage state across the **full original time horizon** while only solving for representative periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Storage settings in ClusterInfo:')\n",
    "print(f'  storage_inter_cluster_linking: {info.storage_inter_cluster_linking}')\n",
    "print(f'  storage_cyclic: {info.storage_cyclic}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize and examine storage behavior\n",
    "solver = fx.solvers.HighsSolver(mip_gap=0.01, log_to_console=False)\n",
    "fs_clustered.optimize(solver)\n",
    "\n",
    "# Check storage charge state\n",
    "if 'Storage|charge_state' in fs_clustered.solution:\n",
    "    charge_state = fs_clustered.solution['Storage|charge_state']\n",
    "    print(f'Charge state shape: {charge_state.shape}')\n",
    "    print(f'Initial charge: {charge_state.values[0]:.1f} MWh')\n",
    "    print(f'Final charge: {charge_state.values[-1]:.1f} MWh')\n",
    "else:\n",
    "    print('No storage in this system')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 8. Multi-Dimensional Support (Periods/Scenarios)\n",
    "\n",
    "When a FlowSystem has multiple **periods** (e.g., investment years) or **scenarios**, each (period, scenario) combination may have **different cluster assignments**.\n",
    "\n",
    "The data structures support this with multi-dimensional arrays:\n",
    "\n",
    "```python\n",
    "# Simple case (no periods/scenarios)\n",
    "cluster_order.dims = ['original_period']\n",
    "timestep_mapping.dims = ['original_time']\n",
    "\n",
    "# Multi-scenario case\n",
    "cluster_order.dims = ['original_period', 'scenario']\n",
    "timestep_mapping.dims = ['original_time', 'scenario']\n",
    "```\n",
    "\n",
    "Helper methods extract per-slice data:\n",
    "```python\n",
    "cluster_structure.get_cluster_order_for_slice(period='2025', scenario='high')\n",
    "cluster_result.get_timestep_mapping_for_slice(scenario='base')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if our system has multi-dimensional clustering\n",
    "print('Multi-dimensional check:')\n",
    "print(f'  cluster_order dims: {cs.cluster_order.dims}')\n",
    "print(f'  has_multi_dims: {cs.has_multi_dims}')\n",
    "\n",
    "# Get cluster order (works for both simple and multi-dim cases)\n",
    "cluster_order = cs.get_cluster_order_for_slice()\n",
    "print(f'  cluster_order shape: {cluster_order.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 9. Expanding Solutions Back to Full Resolution\n",
    "\n",
    "After optimization, `expand_solution()` uses the stored `timestep_mapping` to map typical period results back to the original time horizon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand the solution\n",
    "fs_expanded = fs_clustered.transform.expand_solution()\n",
    "\n",
    "print(f'Clustered timesteps: {len(fs_clustered.timesteps)}')\n",
    "print(f'Expanded timesteps:  {len(fs_expanded.timesteps)}')\n",
    "print(f'Original timesteps:  {len(flow_system.timesteps)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The expanded solution has full time resolution\n",
    "if 'Boiler(Q_th)|flow_rate' in fs_expanded.solution:\n",
    "    flow_clustered = fs_clustered.solution['Boiler(Q_th)|flow_rate']\n",
    "    flow_expanded = fs_expanded.solution['Boiler(Q_th)|flow_rate']\n",
    "\n",
    "    print(f'Clustered flow shape: {flow_clustered.shape}')\n",
    "    print(f'Expanded flow shape:  {flow_expanded.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## 10. Visualizing Typical vs Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get heat demand from original and clustered systems\n",
    "original_demand = flow_system.components['HeatDemand'].inputs[0].fixed_relative_profile.values\n",
    "clustered_demand = fs_clustered.components['HeatDemand'].inputs[0].fixed_relative_profile.values\n",
    "\n",
    "# Reshape into days\n",
    "timesteps_per_day = cs.timesteps_per_cluster\n",
    "n_days = len(original_demand) // timesteps_per_day\n",
    "original_by_day = original_demand[: n_days * timesteps_per_day].reshape(n_days, timesteps_per_day)\n",
    "clustered_by_day = clustered_demand.reshape(cs.n_clusters, timesteps_per_day)\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=1,\n",
    "    subplot_titles=[f'Original: All {n_days} Days', f'Clustered: {cs.n_clusters} Typical Days'],\n",
    "    vertical_spacing=0.15,\n",
    ")\n",
    "\n",
    "hours = np.arange(timesteps_per_day) / 4  # Convert to hours\n",
    "\n",
    "# Plot all original days (faded)\n",
    "for day in range(n_days):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=hours, y=original_by_day[day], mode='lines', line=dict(width=0.5, color='lightblue'), showlegend=False\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "# Plot typical days\n",
    "colors = px.colors.qualitative.Set1\n",
    "for cluster_id in range(cs.n_clusters):\n",
    "    weight = int(cs.cluster_occurrences.sel(cluster=cluster_id).values)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=hours,\n",
    "            y=clustered_by_day[cluster_id],\n",
    "            mode='lines',\n",
    "            name=f'Cluster {cluster_id} (x{weight})',\n",
    "            line=dict(width=2, color=colors[cluster_id % len(colors)]),\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "fig.update_layout(height=600, title='Heat Demand: Original vs Typical Days')\n",
    "fig.update_xaxes(title_text='Hour of Day', row=2, col=1)\n",
    "fig.update_yaxes(title_text='MW')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Module Structure\n",
    "\n",
    "```\n",
    "flixopt.aggregation\n",
    "├── ClusterStructure   # Hierarchical: cluster_order, occurrences\n",
    "├── ClusterResult      # Container: timestep_mapping, weights\n",
    "├── ClusterInfo        # Stored on FlowSystem._cluster_info\n",
    "└── InterClusterLinking # Storage SOC constraints\n",
    "```\n",
    "\n",
    "### Data Flow\n",
    "\n",
    "1. `cluster()` → Creates reduced FlowSystem + stores `ClusterInfo`\n",
    "2. `optimize()` → `InterClusterLinking` adds storage constraints\n",
    "3. `expand_solution()` → Uses `timestep_mapping` to restore full time\n",
    "\n",
    "### Key Formulas\n",
    "\n",
    "| Formula | Description |\n",
    "|---------|-------------|\n",
    "| $\\sum w_t \\cdot c_t$ | Weighted objective function |\n",
    "| $\\sum w_t = N_{\\text{original}}$ | Weight conservation |\n",
    "| $SOC_{d+1} = SOC_d + \\Delta SOC_{c[d]}$ | Inter-cluster storage linking |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
