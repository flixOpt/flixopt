{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Clustering Internals: Weights, TSAM, and Cost Scaling\n",
    "\n",
    "A deep dive into how time series clustering works under the hood.\n",
    "\n",
    "This notebook covers:\n",
    "\n",
    "- **Cluster weights**: How operational costs are scaled to represent the full time horizon\n",
    "- **TSAM integration**: How the Time Series Aggregation Module performs clustering\n",
    "- **Typical periods**: Visualizing representative vs original time series\n",
    "- **Storage handling**: Inter-period linking and cyclic constraints\n",
    "- **The `_aggregation_info` structure**: Internal data for expansion and analysis\n",
    "\n",
    "!!! note \"Prerequisites\"\n",
    "    This notebook assumes familiarity with [08c-clustering](08c-clustering.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import flixopt as fx\n",
    "\n",
    "fx.CONFIG.notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the district heating system\n",
    "data_file = Path('data/district_heating_system.nc4')\n",
    "if not data_file.exists():\n",
    "    from data.generate_example_systems import create_district_heating_system\n",
    "\n",
    "    fs = create_district_heating_system()\n",
    "    fs.to_netcdf(data_file)\n",
    "\n",
    "flow_system = fx.FlowSystem.from_netcdf(data_file)\n",
    "print(f'Loaded: {len(flow_system.timesteps)} timesteps ({len(flow_system.timesteps) / 96:.0f} days)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clustered system for analysis\n",
    "fs_clustered = flow_system.transform.aggregate(\n",
    "    method='tsam',\n",
    "    n_representatives=8,\n",
    "    cluster_duration='1D',\n",
    "    time_series_for_high_peaks=['HeatDemand(Q_th)|fixed_relative_profile'],\n",
    ")\n",
    "\n",
    "print(f'Clustered: {len(fs_clustered.timesteps)} timesteps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 1. The `_aggregation_info` Structure\n",
    "\n",
    "After clustering, the FlowSystem stores metadata in `_aggregation_info` that enables:\n",
    "- Expanding solutions back to full resolution\n",
    "- Understanding which original days map to which clusters\n",
    "- Weighting costs correctly in the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = fs_clustered._aggregation_info\n",
    "\n",
    "print('AggregationInfo structure:')\n",
    "print(f'  backend_name: {info.backend_name}')\n",
    "print(f'  storage_inter_cluster_linking: {info.storage_inter_cluster_linking}')\n",
    "print(f'  storage_cyclic: {info.storage_cyclic}')\n",
    "\n",
    "cs = info.result.cluster_structure\n",
    "print('\\nClusterStructure:')\n",
    "print(f'  n_clusters: {cs.n_clusters}')\n",
    "print(f'  timesteps_per_cluster: {cs.timesteps_per_cluster}')\n",
    "print(f'  cluster_order shape: {cs.cluster_order.shape}')\n",
    "print(f'  cluster_occurrences: {dict(cs.cluster_occurrences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Cluster Order: Mapping Days to Clusters\n",
    "\n",
    "The `cluster_order` array shows which cluster each original day belongs to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = fs_clustered._aggregation_info\n",
    "cs = info.result.cluster_structure\n",
    "cluster_order = cs.cluster_order.values\n",
    "n_original_days = len(cluster_order)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "days_df = pd.DataFrame(\n",
    "    {\n",
    "        'Day': range(1, n_original_days + 1),\n",
    "        'Cluster': cluster_order,\n",
    "        'Date': pd.date_range('2020-01-01', periods=n_original_days, freq='D'),\n",
    "    }\n",
    ")\n",
    "days_df['Weekday'] = days_df['Date'].dt.day_name()\n",
    "\n",
    "print(f'Original days: {n_original_days}')\n",
    "print(f'Number of clusters: {cs.n_clusters}')\n",
    "print('\\nFirst 14 days:')\n",
    "print(days_df.head(14).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster assignment as a heatmap\n",
    "fig = px.bar(\n",
    "    days_df,\n",
    "    x='Day',\n",
    "    y=[1] * len(days_df),\n",
    "    color='Cluster',\n",
    "    color_continuous_scale='Viridis',\n",
    "    title='Cluster Assignment by Day',\n",
    "    labels={'y': ''},\n",
    ")\n",
    "fig.update_layout(height=250, yaxis_visible=False, coloraxis_colorbar_title='Cluster')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 2. Cluster Weights: Scaling Operational Costs\n",
    "\n",
    "When we optimize over 8 typical days instead of 31, the operational costs for each typical day\n",
    "must be **scaled** to represent all the days it represents.\n",
    "\n",
    "### The `cluster_weight` Property\n",
    "\n",
    "The clustered FlowSystem has a `cluster_weight` that stores the weight for each timestep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cluster_weight is stored on the FlowSystem\n",
    "print('cluster_weight structure:')\n",
    "print(fs_clustered.cluster_weight)\n",
    "print(f'\\nShape: {fs_clustered.cluster_weight.shape}')\n",
    "print(f'Sum of weights: {fs_clustered.cluster_weight.sum().item():.0f}')\n",
    "print(f'Expected (original timesteps): {len(flow_system.timesteps)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster occurrences (how many original days each cluster represents)\n",
    "info = fs_clustered._aggregation_info\n",
    "cs = info.result.cluster_structure\n",
    "cluster_occurrences = dict(cs.cluster_occurrences)\n",
    "\n",
    "print('Cluster occurrences (days represented by each typical day):')\n",
    "for cluster_id, count in sorted(cluster_occurrences.items()):\n",
    "    print(f'  Cluster {cluster_id}: {count} days (weight = {count})')\n",
    "\n",
    "print(f'\\nTotal: {sum(cluster_occurrences.values())} days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize weights across the reduced timesteps\n",
    "info = fs_clustered._aggregation_info\n",
    "cs = info.result.cluster_structure\n",
    "weights = fs_clustered.cluster_weight.values\n",
    "timesteps_per_day = cs.timesteps_per_cluster\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(weights))),\n",
    "        y=weights,\n",
    "        mode='lines',\n",
    "        name='Cluster Weight',\n",
    "        line=dict(width=1),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add vertical lines at day boundaries\n",
    "for i in range(1, cs.n_clusters):\n",
    "    fig.add_vline(x=i * timesteps_per_day, line_dash='dash', line_color='gray', opacity=0.5)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=300,\n",
    "    title='Cluster Weight per Timestep (Each Typical Day Has Uniform Weight)',\n",
    "    xaxis_title='Timestep Index',\n",
    "    yaxis_title='Weight',\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### How Weights Affect the Objective Function\n",
    "\n",
    "The objective function multiplies operational costs by the cluster weight:\n",
    "\n",
    "$$\\text{Objective} = \\sum_{t \\in \\text{typical}} w_t \\cdot c_t$$\n",
    "\n",
    "Where:\n",
    "- $w_t$ = cluster weight for timestep $t$ (= number of original days this cluster represents)\n",
    "- $c_t$ = operational cost at timestep $t$\n",
    "\n",
    "This ensures that a typical day representing 7 similar days contributes 7× more to the objective\n",
    "than a typical day representing only 1 day (e.g., a peak day)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate how weights are applied (conceptually)\n",
    "solver = fx.solvers.HighsSolver(mip_gap=0.01, log_to_console=False)\n",
    "fs_clustered.optimize(solver)\n",
    "\n",
    "# The 'costs' solution is already weighted\n",
    "total_cost = fs_clustered.solution['costs'].item()\n",
    "\n",
    "# We can also access the per-timestep costs\n",
    "costs_per_timestep = fs_clustered.solution['costs(temporal)|per_timestep']\n",
    "\n",
    "print(f'Total cost (weighted): {total_cost:,.0f} €')\n",
    "print(f'\\nCosts per timestep shape: {costs_per_timestep.shape}')\n",
    "print(f'Sum of weighted costs: {(costs_per_timestep * fs_clustered.cluster_weight).sum().item():,.0f} €')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 3. TSAM Integration: The Clustering Algorithm\n",
    "\n",
    "flixopt uses the [TSAM](https://github.com/FZJ-IEK3-VSA/tsam) (Time Series Aggregation Module) \n",
    "package for clustering. TSAM uses k-means clustering to group similar time periods.\n",
    "\n",
    "### The Clustering Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the TSAM clustering object\n",
    "clustering = info['clustering']\n",
    "\n",
    "print(f'Clustering type: {type(clustering).__name__}')\n",
    "print(f'\\nTSAM aggregation object: {type(clustering.tsam).__name__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The TSAM object contains the clustering results\n",
    "tsam = clustering.tsam\n",
    "\n",
    "print('TSAM typical periods (centroids):')\n",
    "print(tsam.typicalPeriods.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster centers vs original data\n",
    "print('\\nOriginal time series used for clustering:')\n",
    "print(f'Shape: {tsam.normalizedPeriodlyProfiles.shape}')\n",
    "print(f'Columns: {list(tsam.normalizedPeriodlyProfiles.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Visualizing Typical Periods vs Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get heat demand from original and clustered systems\n",
    "original_demand = flow_system.components['HeatDemand'].inputs[0].fixed_relative_profile.values\n",
    "clustered_demand = fs_clustered.components['HeatDemand'].inputs[0].fixed_relative_profile.values\n",
    "\n",
    "# Reshape original demand into days\n",
    "timesteps_per_day = 96  # 15-minute resolution\n",
    "n_days = len(original_demand) // timesteps_per_day\n",
    "original_by_day = original_demand[: n_days * timesteps_per_day].reshape(n_days, timesteps_per_day)\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=1,\n",
    "    subplot_titles=['Original: All 31 Days', f'Clustered: {info[\"n_clusters\"]} Typical Days'],\n",
    "    vertical_spacing=0.15,\n",
    ")\n",
    "\n",
    "# Plot all original days (faded)\n",
    "hours = np.arange(timesteps_per_day) / 4  # Convert to hours\n",
    "for day in range(n_days):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=hours,\n",
    "            y=original_by_day[day],\n",
    "            mode='lines',\n",
    "            line=dict(width=0.5, color='lightblue'),\n",
    "            showlegend=False,\n",
    "            hoverinfo='skip',\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "# Plot typical days (bold colors)\n",
    "colors = px.colors.qualitative.Set1\n",
    "n_clusters = info['n_clusters']\n",
    "clustered_by_day = clustered_demand.reshape(n_clusters, timesteps_per_day)\n",
    "\n",
    "for cluster_id in range(n_clusters):\n",
    "    weight = cluster_occurrences.get(cluster_id, cluster_occurrences.get(np.int32(cluster_id), 1))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=hours,\n",
    "            y=clustered_by_day[cluster_id],\n",
    "            mode='lines',\n",
    "            name=f'Cluster {cluster_id} (×{weight})',\n",
    "            line=dict(width=2, color=colors[cluster_id % len(colors)]),\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "fig.update_layout(height=600, title='Heat Demand: Original vs Typical Days')\n",
    "fig.update_xaxes(title_text='Hour of Day', row=2, col=1)\n",
    "fig.update_yaxes(title_text='MW', row=1, col=1)\n",
    "fig.update_yaxes(title_text='MW', row=2, col=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 4. Storage Handling in Clustering\n",
    "\n",
    "Storage behavior across typical periods requires special handling:\n",
    "\n",
    "### Cyclic Constraint (`storage_cyclic=True`)\n",
    "\n",
    "When enabled (default), the storage state at the end of each typical period must equal \n",
    "the state at the beginning. This prevents the optimizer from \"cheating\" by starting \n",
    "with a full storage and ending empty.\n",
    "\n",
    "### Inter-Period Linking\n",
    "\n",
    "The `storage_inter_period_linking` option controls whether storage states are linked \n",
    "across typical periods to simulate long-term storage behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Storage settings:')\n",
    "print(f'  storage_cyclic: {info[\"storage_cyclic\"]}')\n",
    "print(f'  storage_inter_period_linking: {info[\"storage_inter_period_linking\"]}')\n",
    "\n",
    "# Show storage charge state in clustered solution\n",
    "charge_state = fs_clustered.solution['Storage|charge_state']\n",
    "print(f'\\nCharge state shape: {charge_state.shape}')\n",
    "print(f'Initial charge: {charge_state.values[0]:.1f} MWh')\n",
    "print(f'Final charge: {charge_state.values[-1]:.1f} MWh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize storage behavior across typical periods\n",
    "fig = go.Figure()\n",
    "\n",
    "timesteps_per_day = info['timesteps_per_cluster']\n",
    "charge_values = charge_state.values\n",
    "\n",
    "# Plot each typical day's storage trajectory\n",
    "colors = px.colors.qualitative.Set1\n",
    "for cluster_id in range(info['n_clusters']):\n",
    "    start_idx = cluster_id * timesteps_per_day\n",
    "    end_idx = start_idx + timesteps_per_day + 1  # Include endpoint\n",
    "\n",
    "    if end_idx <= len(charge_values):\n",
    "        hours = np.arange(timesteps_per_day + 1) / 4\n",
    "        weight = cluster_occurrences.get(cluster_id, cluster_occurrences.get(np.int32(cluster_id), 1))\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=hours,\n",
    "                y=charge_values[start_idx:end_idx],\n",
    "                mode='lines',\n",
    "                name=f'Cluster {cluster_id} (×{weight})',\n",
    "                line=dict(width=2, color=colors[cluster_id % len(colors)]),\n",
    "            )\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    title='Storage Charge State by Typical Period (Cyclic: Start = End)',\n",
    "    xaxis_title='Hour of Day',\n",
    "    yaxis_title='Charge State [MWh]',\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 5. The `weights` Property: Unified Access\n",
    "\n",
    "The FlowSystem provides a unified `weights` property that combines all weighting factors\n",
    "(aggregation weights, scenario weights, period weights) into a single xarray structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The weights property provides unified access\n",
    "weights = fs_clustered.weights\n",
    "\n",
    "print('FlowSystem weights structure:')\n",
    "print(f'  Type: {type(weights).__name__}')\n",
    "print(f'  temporal: {weights.temporal}')\n",
    "print(f'  aggregation_weight: {weights.aggregation_weight}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare weights for original vs clustered systems\n",
    "print('Original system weights:')\n",
    "print(f'  temporal: {flow_system.weights.temporal}')\n",
    "print(f'  aggregation_weight: {flow_system.weights.aggregation_weight}')\n",
    "\n",
    "print('\\nClustered system weights:')\n",
    "print(f'  temporal: {fs_clustered.weights.temporal}')\n",
    "print(f'  aggregation_weight (cluster_weight): sum = {fs_clustered.weights.aggregation_weight.sum().item():.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## 6. Time Series Weights in Clustering\n",
    "\n",
    "You can influence which time series are prioritized during clustering using the `weights` parameter.\n",
    "By default, all time series are weighted equally, but you may want to:\n",
    "\n",
    "- Give higher weight to demand profiles (more important to capture accurately)\n",
    "- Give lower weight to price signals (less critical for sizing)\n",
    "\n",
    "### Automatic Weight Calculation\n",
    "\n",
    "flixopt automatically calculates weights based on `clustering_group` attributes to avoid\n",
    "double-counting correlated time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the time series used for clustering and their weights\n",
    "if hasattr(clustering, 'tsam') and hasattr(clustering.tsam, 'normalizedPeriodlyProfiles'):\n",
    "    ts_names = list(clustering.tsam.normalizedPeriodlyProfiles.columns)\n",
    "    print('Time series used for clustering:')\n",
    "    for name in ts_names:\n",
    "        print(f'  - {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## 7. Peak Forcing: Ensuring Extreme Periods\n",
    "\n",
    "The `time_series_for_high_peaks` parameter forces inclusion of periods containing peak values.\n",
    "This is critical for proper component sizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find which cluster contains the peak demand day\n",
    "original_demand = flow_system.components['HeatDemand'].inputs[0].fixed_relative_profile.values\n",
    "daily_max = original_demand.reshape(-1, 96).max(axis=1)\n",
    "\n",
    "peak_day = np.argmax(daily_max)\n",
    "peak_cluster = cluster_order[peak_day]\n",
    "peak_value = daily_max[peak_day]\n",
    "\n",
    "# Get weight for the peak cluster\n",
    "peak_weight = cluster_occurrences.get(peak_cluster, cluster_occurrences.get(np.int32(peak_cluster), 1))\n",
    "\n",
    "print(f'Peak demand day: Day {peak_day + 1} (0-indexed: {peak_day})')\n",
    "print(f'Peak value: {peak_value:.1f} MW')\n",
    "print(f'Assigned to cluster: {peak_cluster}')\n",
    "print(f'Cluster {peak_cluster} represents {peak_weight} day(s)')\n",
    "\n",
    "# The peak day should be in a cluster with weight 1 (unique)\n",
    "if peak_weight == 1:\n",
    "    print('\\\\n✓ Peak day is isolated in its own cluster (weight=1) - good!')\n",
    "else:\n",
    "    print(f'\\\\n⚠ Peak day shares cluster with {peak_weight - 1} other day(s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You learned about the internal mechanics of clustering:\n",
    "\n",
    "1. **`_cluster_info`**: Contains all metadata for expansion and analysis\n",
    "2. **Cluster weights**: Scale operational costs so each typical period represents its original days\n",
    "3. **TSAM integration**: k-means clustering groups similar time periods\n",
    "4. **Storage handling**: Cyclic constraints ensure realistic storage behavior\n",
    "5. **Peak forcing**: Guarantees extreme periods are captured for proper sizing\n",
    "\n",
    "### Key Formulas\n",
    "\n",
    "**Weighted objective:**\n",
    "$$\\text{Objective} = \\sum_{t \\in \\text{typical}} w_t \\cdot c_t$$\n",
    "\n",
    "**Weight conservation:**\n",
    "$$\\sum_{t \\in \\text{typical}} w_t = |\\text{original timesteps}|$$\n",
    "\n",
    "### When to Customize\n",
    "\n",
    "| Scenario | Solution |\n",
    "|----------|----------|\n",
    "| Peak days not captured | Add `time_series_for_high_peaks` |\n",
    "| Minimum periods important | Add `time_series_for_low_peaks` |\n",
    "| Specific profiles more important | Use custom `weights` dict |\n",
    "| Storage behaves unrealistically | Check `storage_cyclic` setting |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
