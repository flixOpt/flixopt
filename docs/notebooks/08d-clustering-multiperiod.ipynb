{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {},
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"0\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Multi-Period Clustering with `cluster()`\\n\",\n",
    "    \"\\n\",\n",
    "    \"Combine time series clustering with multi-period investment optimization.\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook demonstrates:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- **Multi-period modeling**: Optimize investments across multiple planning periods (years)\\n\",\n",
    "    \"- **Scenario analysis**: Handle demand uncertainty with weighted scenarios\\n\",\n",
    "    \"- **Clustering per period**: Apply typical-period clustering independently for each period/scenario\\n\",\n",
    "    \"- **Scalability**: Reduce computational complexity for long-horizon planning\\n\",\n",
    "    \"\\n\",\n",
    "    \"!!! note \\\"Requirements\\\"\\n\",\n",
    "    \"    This notebook requires the `tsam` package: `pip install tsam`\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"1\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import timeit\\n\",\n",
    "    \"\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import plotly.express as px\\n\",\n",
    "    \"\\n\",\n",
    "    \"import flixopt as fx\\n\",\n",
    "    \"\\n\",\n",
    "    \"fx.CONFIG.notebook()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"2\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Create the Multi-Period System\\n\",\n",
    "    \"\\n\",\n",
    "    \"We use a multi-period heating system with:\\n\",\n",
    "    \"- **3 planning periods** (years 2024, 2025, 2026)\\n\",\n",
    "    \"- **2 scenarios** (high demand 30%, low demand 70%)\\n\",\n",
    "    \"- **2 weeks** at hourly resolution (336 timesteps)\\n\",\n",
    "    \"\\n\",\n",
    "    \"This represents a capacity expansion problem where we optimize component sizes once,\\n\",\n",
    "    \"but operations are simulated across multiple future years and demand scenarios.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"3\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from data.generate_example_systems import create_multiperiod_system\\n\",\n",
    "    \"\\n\",\n",
    "    \"flow_system = create_multiperiod_system()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f'Timesteps: {len(flow_system.timesteps)} ({len(flow_system.timesteps) // 24} days)')\\n\",\n",
    "    \"print(f'Periods: {list(flow_system.periods.values)}')\\n\",\n",
    "    \"print(f'Scenarios: {list(flow_system.scenarios.values)}')\\n\",\n",
    "    \"print(f'Scenario weights: {flow_system.scenario_weights.values}')\\n\",\n",
    "    \"print(f'\\\\nComponents: {list(flow_system.components.keys())}')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"4\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Selecting a Subset with `transform.isel()`\\n\",\n",
    "    \"\\n\",\n",
    "    \"For demonstration purposes, we'll use only the first week of data.\\n\",\n",
    "    \"The `isel()` method (index select) lets you slice FlowSystems by time:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"5\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Select first week only (168 hours)\\n\",\n",
    "    \"flow_system = flow_system.transform.isel(time=slice(0, 168))\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f'After isel: {len(flow_system.timesteps)} timesteps ({len(flow_system.timesteps) // 24} days)')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"6\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Visualize demand scenarios\\n\",\n",
    "    \"heat_demand = flow_system.components['Building'].inputs[0].fixed_relative_profile\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig = px.line(\\n\",\n",
    "    \"    heat_demand.to_dataframe('value').reset_index(), x='time', y='value', facet_col='period', facet_row='scenario'\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig.update_layout(\\n\",\n",
    "    \"    height=350,\\n\",\n",
    "    \"    title='Heat Demand by Scenario (One Week)',\\n\",\n",
    "    \"    xaxis_title='Time',\\n\",\n",
    "    \"    yaxis_title='Heat Demand [kW]',\\n\",\n",
    "    \")\\n\",\n",
    "    \"fig.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"7\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Full Optimization (Baseline)\\n\",\n",
    "    \"\\n\",\n",
    "    \"First, solve the complete problem with all timesteps across all periods and scenarios:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"8\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"solver = fx.solvers.HighsSolver(mip_gap=0.01)\\n\",\n",
    "    \"\\n\",\n",
    "    \"start = timeit.default_timer()\\n\",\n",
    "    \"fs_full = flow_system.copy()\\n\",\n",
    "    \"fs_full.name = 'Full Optimization'\\n\",\n",
    "    \"fs_full.optimize(solver)\\n\",\n",
    "    \"time_full = timeit.default_timer() - start\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f'Full optimization: {time_full:.2f} seconds')\\n\",\n",
    "    \"print(f'Total cost (objective): {fs_full.solution[\\\"objective\\\"].item():,.0f} €')\\n\",\n",
    "    \"print('\\\\nOptimized sizes:')\\n\",\n",
    "    \"for name, size in fs_full.statistics.sizes.items():\\n\",\n",
    "    \"    print(f'  {name}: {size.max().item():.1f}')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"9\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Multi-Period Clustering with `cluster()`\\n\",\n",
    "    \"\\n\",\n",
    "    \"When applied to a multi-period system, `cluster()` clusters **each period/scenario combination independently**.\\n\",\n",
    "    \"This is because demand patterns and optimal operations may differ across:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- **Periods**: Different years may have different characteristics\\n\",\n",
    "    \"- **Scenarios**: High vs low demand scenarios need different representative days\\n\",\n",
    "    \"\\n\",\n",
    "    \"The investment decisions (sizes) remain consistent across all periods and scenarios,\\n\",\n",
    "    \"while the operational patterns are optimized for each cluster.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"10\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": \"import tsam\\n\\nstart = timeit.default_timer()\\n\\n# Force inclusion of peak demand periods\\npeak_series = ['Building(Heat)|fixed_relative_profile']\\n\\n# Cluster to 3 typical days (from 7 days)\\nfs_clustered = flow_system.transform.cluster(\\n    n_clusters=3,\\n    cluster_duration='1D',\\n    extremes=tsam.ExtremeConfig(max_value=peak_series),\\n)\\n\\ntime_clustering = timeit.default_timer() - start\\n\\nprint(f'Clustering time: {time_clustering:.2f} seconds')\\nprint(f'Reduced: {len(flow_system.timesteps)} → {len(fs_clustered.timesteps)} timesteps per period')\\nprint('Total problem reduction: 7 days × 3 periods × 2 scenarios → 3 days × 3 × 2')\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"11\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Optimize the reduced system\\n\",\n",
    "    \"start = timeit.default_timer()\\n\",\n",
    "    \"fs_clustered.optimize(solver)\\n\",\n",
    "    \"time_clustered = timeit.default_timer() - start\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f'Clustered optimization: {time_clustered:.2f} seconds')\\n\",\n",
    "    \"print(f'Total cost (objective): {fs_clustered.solution[\\\"objective\\\"].item():,.0f} €')\\n\",\n",
    "    \"print(f'\\\\nSpeedup vs full: {time_full / (time_clustering + time_clustered):.1f}x')\\n\",\n",
    "    \"print('\\\\nOptimized sizes:')\\n\",\n",
    "    \"for name, size in fs_clustered.statistics.sizes.items():\\n\",\n",
    "    \"    print(f'  {name}: {size.max().item():.1f}')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"12\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Visualize Clustering Quality\\n\",\n",
    "    \"\\n\",\n",
    "    \"The `.plot` accessor provides built-in visualizations with automatic faceting by period and scenario:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"13\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": \"# Visualize clustering with heatmap - automatically faceted by period and scenario\\nfs_clustered.clustering.plot.heatmap()\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"14\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": \"# View clustering quality metrics - shows RMSE and MAE per time series\\nfs_clustered.clustering.metrics.to_dataframe().style.format('{:.3f}')\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"15\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Heatmap shows cluster assignments - faceted by period and scenario\\n\",\n",
    "    \"fs_clustered.clustering.plot.heatmap()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"16\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Understand the Cluster Structure\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's inspect how days were grouped into clusters:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"17\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": \"clustering = fs_clustered.clustering\\n\\nprint('Clustering Configuration:')\\nprint(f'  Typical periods (clusters): {clustering.n_clusters}')\\nprint(f'  Timesteps per cluster: {clustering.timesteps_per_cluster}')\\n\\n# The cluster_assignments shows which cluster each original day belongs to\\ncluster_order = clustering.cluster_assignments.values\\nday_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\\n\\nprint('\\\\nCluster assignments per day (first period/scenario):')\\n# For multi-dimensional, take the first slice\\nif cluster_order.ndim > 1:\\n    cluster_order_1d = cluster_order[:, 0, 0] if cluster_order.ndim == 3 else cluster_order[:, 0]\\nelse:\\n    cluster_order_1d = cluster_order\\n\\nfor i, cluster_id in enumerate(cluster_order_1d):\\n    print(f'  {day_names[i]}: Cluster {cluster_id}')\\n\\n# Cluster weights (how many original days each cluster represents)\\nprint('\\\\nCluster weights (occurrence count):')\\nprint(clustering.cluster_weights.values)\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"18\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Two-Stage Workflow for Multi-Period\\n\",\n",
    "    \"\\n\",\n",
    "    \"For investment optimization across multiple periods, the recommended workflow is:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. **Stage 1**: Fast sizing with clustering (reduced timesteps)\\n\",\n",
    "    \"2. **Stage 2**: Fix sizes and run full-resolution dispatch\\n\",\n",
    "    \"\\n\",\n",
    "    \"This gives accurate investment decisions while maintaining computational tractability.\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Safety Margin Rationale\\n\",\n",
    "    \"\\n\",\n",
    "    \"A 10% safety margin is applied to compensate for:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- **Peak underestimation**: Clustering averages similar days, potentially underestimating true peak demands\\n\",\n",
    "    \"- **Temporal detail loss**: Representative periods may miss short-duration extreme events\\n\",\n",
    "    \"- **Scenario averaging**: Weighted scenarios smooth out worst-case conditions\\n\",\n",
    "    \"\\n\",\n",
    "    \"For critical applications, consider 15-20% margins or validate with full-resolution runs.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"19\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Stage 1 already done - apply safety margin\\n\",\n",
    "    \"SAFETY_MARGIN = 1.10  # 10% buffer for multi-period uncertainty\\n\",\n",
    "    \"\\n\",\n",
    "    \"sizes_with_margin = {name: size.max().item() * SAFETY_MARGIN for name, size in fs_clustered.statistics.sizes.items()}\\n\",\n",
    "    \"\\n\",\n",
    "    \"print('Stage 1: Sizing with clustering')\\n\",\n",
    "    \"print(f'  Time: {time_clustering + time_clustered:.2f} seconds')\\n\",\n",
    "    \"print(f'  Cost estimate: {fs_clustered.solution[\\\"objective\\\"].item():,.0f} €')\\n\",\n",
    "    \"print(f'\\\\nSizes with {(SAFETY_MARGIN - 1) * 100:.0f}% safety margin:')\\n\",\n",
    "    \"for name, size in sizes_with_margin.items():\\n\",\n",
    "    \"    original = fs_clustered.statistics.sizes[name].max().item()\\n\",\n",
    "    \"    print(f'  {name}: {original:.1f} → {size:.1f}')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"20\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Stage 2: Full resolution dispatch with fixed sizes\\n\",\n",
    "    \"print('Stage 2: Full resolution dispatch')\\n\",\n",
    "    \"start = timeit.default_timer()\\n\",\n",
    "    \"\\n\",\n",
    "    \"fs_dispatch = flow_system.transform.fix_sizes(sizes_with_margin)\\n\",\n",
    "    \"fs_dispatch.name = 'Two-Stage'\\n\",\n",
    "    \"fs_dispatch.optimize(solver)\\n\",\n",
    "    \"\\n\",\n",
    "    \"time_dispatch = timeit.default_timer() - start\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f'  Time: {time_dispatch:.2f} seconds')\\n\",\n",
    "    \"print(f'  Actual cost: {fs_dispatch.solution[\\\"objective\\\"].item():,.0f} €')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Total comparison\\n\",\n",
    "    \"total_two_stage = time_clustering + time_clustered + time_dispatch\\n\",\n",
    "    \"print(f'\\\\nTotal two-stage time: {total_two_stage:.2f} seconds')\\n\",\n",
    "    \"print(f'Speedup vs full: {time_full / total_two_stage:.1f}x')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"21\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Compare Results Across Methods\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"22\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"results = {\\n\",\n",
    "    \"    'Full (baseline)': {\\n\",\n",
    "    \"        'Time [s]': time_full,\\n\",\n",
    "    \"        'Cost [€]': fs_full.solution['objective'].item(),\\n\",\n",
    "    \"        'Boiler': fs_full.statistics.sizes['Boiler(Heat)'].max().item(),\\n\",\n",
    "    \"        'Storage': fs_full.statistics.sizes['ThermalStorage'].max().item(),\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    'Clustered (3 days)': {\\n\",\n",
    "    \"        'Time [s]': time_clustering + time_clustered,\\n\",\n",
    "    \"        'Cost [€]': fs_clustered.solution['objective'].item(),\\n\",\n",
    "    \"        'Boiler': fs_clustered.statistics.sizes['Boiler(Heat)'].max().item(),\\n\",\n",
    "    \"        'Storage': fs_clustered.statistics.sizes['ThermalStorage'].max().item(),\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    'Two-Stage': {\\n\",\n",
    "    \"        'Time [s]': total_two_stage,\\n\",\n",
    "    \"        'Cost [€]': fs_dispatch.solution['objective'].item(),\\n\",\n",
    "    \"        'Boiler': sizes_with_margin['Boiler(Heat)'],\\n\",\n",
    "    \"        'Storage': sizes_with_margin['ThermalStorage'],\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"comparison = pd.DataFrame(results).T\\n\",\n",
    "    \"baseline_cost = comparison.loc['Full (baseline)', 'Cost [€]']\\n\",\n",
    "    \"baseline_time = comparison.loc['Full (baseline)', 'Time [s]']\\n\",\n",
    "    \"comparison['Cost Gap [%]'] = ((comparison['Cost [€]'] - baseline_cost) / abs(baseline_cost) * 100).round(2)\\n\",\n",
    "    \"comparison['Speedup'] = (baseline_time / comparison['Time [s]']).round(1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"comparison.style.format(\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        'Time [s]': '{:.2f}',\\n\",\n",
    "    \"        'Cost [€]': '{:,.0f}',\\n\",\n",
    "    \"        'Boiler': '{:.1f}',\\n\",\n",
    "    \"        'Storage': '{:.0f}',\\n\",\n",
    "    \"        'Cost Gap [%]': '{:.2f}',\\n\",\n",
    "    \"        'Speedup': '{:.1f}x',\\n\",\n",
    "    \"    }\\n\",\n",
    "    \")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"23\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Visualize Optimization Results\\n\",\n",
    "    \"\\n\",\n",
    "    \"Use the built-in statistics plotting to compare results across periods and scenarios:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"24\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Plot flow rates with automatic faceting by period and scenario\\n\",\n",
    "    \"fs_full.statistics.plot.flows(component='Boiler')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"25\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Side-by-side comparison using the Comparison class\\n\",\n",
    "    \"comp = fx.Comparison([fs_full, fs_dispatch])\\n\",\n",
    "    \"comp.statistics.plot.balance('Heat')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"26\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Expand Clustered Solution to Full Resolution\\n\",\n",
    "    \"\\n\",\n",
    "    \"Use `expand()` to map the clustered results back to all original timesteps:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"27\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Expand the clustered solution\\n\",\n",
    "    \"fs_expanded = fs_clustered.transform.expand()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f'Expanded: {len(fs_clustered.timesteps)} → {len(fs_expanded.timesteps)} timesteps')\\n\",\n",
    "    \"print(f'Cost (objective): {fs_expanded.solution[\\\"objective\\\"].item():,.0f} €')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"28\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Compare expanded solution - shows the repeated cluster patterns\\n\",\n",
    "    \"fs_expanded.statistics.plot.flows(component='Boiler')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"29\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Key Considerations for Multi-Period Clustering\\n\",\n",
    "    \"\\n\",\n",
    "    \"### 1. Independent Clustering per Period/Scenario\\n\",\n",
    "    \"\\n\",\n",
    "    \"Each period and scenario combination is clustered independently because:\\n\",\n",
    "    \"- Demand patterns may differ across years (growth, seasonality)\\n\",\n",
    "    \"- Scenarios represent different futures that shouldn't be mixed\\n\",\n",
    "    \"- Investment decisions must be robust across all combinations\\n\",\n",
    "    \"\\n\",\n",
    "    \"### 2. Safety Margins\\n\",\n",
    "    \"\\n\",\n",
    "    \"Multi-period systems often warrant larger safety margins (10-15%) because:\\n\",\n",
    "    \"- More uncertainty across multiple years\\n\",\n",
    "    \"- Investments made once must work for all periods\\n\",\n",
    "    \"- Scenario weights may not perfectly represent actual outcomes\\n\",\n",
    "    \"\\n\",\n",
    "    \"### 3. Computational Benefits\\n\",\n",
    "    \"\\n\",\n",
    "    \"Clustering becomes more valuable as problem size grows:\\n\",\n",
    "    \"\\n\",\n",
    "    \"| Scenario | Full Problem | With Clustering |\\n\",\n",
    "    \"|----------|--------------|----------------|\\n\",\n",
    "    \"| 1 period, 1 scenario, 365 days | 8,760 timesteps | ~730 (10 typical days) |\\n\",\n",
    "    \"| 3 periods, 2 scenarios, 365 days | 52,560 timesteps | ~4,380 |\\n\",\n",
    "    \"| 10 periods, 3 scenarios, 365 days | 262,800 timesteps | ~21,900 |\\n\",\n",
    "    \"\\n\",\n",
    "    \"The speedup factor increases with problem size.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"30\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": \"## Summary\\n\\nYou learned how to:\\n\\n- Load **multi-period systems** with periods and scenarios\\n- Use **`transform.isel()`** to select time subsets\\n- Apply **`cluster()`** to multi-dimensional FlowSystems\\n- **Access clustering metadata** via `fs.clustering` (metrics, cluster_assignments)\\n- Use the **two-stage workflow** for robust investment optimization\\n- **Expand solutions** back to full resolution with `expand()`\\n\\n### Key Takeaways\\n\\n1. **Clustering is applied per period/scenario**: Each combination gets independent typical periods\\n2. **Investments are shared**: Component sizes are optimized once across all periods/scenarios\\n3. **Use larger safety margins**: Multi-period uncertainty warrants 10-15% buffers\\n4. **Two-stage is recommended**: Fast sizing with clustering, accurate dispatch at full resolution\\n5. **Built-in plotting**: Use `clustering.plot.heatmap()` for cluster visualization\\n\\n### API Reference\\n\\n```python\\nimport tsam\\n\\n# Load multi-period system\\nfs = fx.FlowSystem.from_netcdf('multiperiod_system.nc4')\\n\\n# Select time subset (optional)\\nfs = fs.transform.isel(time=slice(0, 168))  # First 168 timesteps\\n\\n# Cluster (applies per period/scenario)\\nfs_clustered = fs.transform.cluster(\\n    n_clusters=10,\\n    cluster_duration='1D',\\n    extremes=tsam.ExtremeConfig(max_value=['Demand(Flow)|fixed_relative_profile']),\\n)\\n\\n# Access clustering metadata\\nfs_clustered.clustering.n_clusters           # Number of clusters\\nfs_clustered.clustering.cluster_assignments  # Which cluster each original segment belongs to\\nfs_clustered.clustering.cluster_weights      # How many original segments per cluster\\nfs_clustered.clustering.metrics              # RMSE, MAE per time series\\nfs_clustered.clustering.plot.heatmap()       # Visualize cluster structure\\n\\n# Two-stage workflow\\nfs_clustered.optimize(solver)\\nsizes = {k: v.max().item() * 1.10 for k, v in fs_clustered.statistics.sizes.items()}\\nfs_dispatch = fs.transform.fix_sizes(sizes)\\nfs_dispatch.optimize(solver)\\n\\n# Visualize results\\nfs_dispatch.statistics.plot.flows(component='Boiler')\\n```\"\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\",\n",
    "   \"version\": \"3.11\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
