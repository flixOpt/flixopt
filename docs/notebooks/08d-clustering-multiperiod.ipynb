{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Multi-Period Clustering with `cluster()`\n",
    "\n",
    "Combine time series clustering with multi-period investment optimization.\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "- **Multi-period modeling**: Optimize investments across multiple planning periods (years)\n",
    "- **Scenario analysis**: Handle demand uncertainty with weighted scenarios\n",
    "- **Clustering per period**: Apply typical-period clustering independently for each period/scenario\n",
    "- **Scalability**: Reduce computational complexity for long-horizon planning\n",
    "\n",
    "!!! note \"Requirements\"\n",
    "    This notebook requires the `tsam` package: `pip install tsam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "import flixopt as fx\n",
    "\n",
    "fx.CONFIG.notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Create the Multi-Period System\n",
    "\n",
    "We use a multi-period heating system with:\n",
    "- **3 planning periods** (years 2024, 2025, 2026)\n",
    "- **2 scenarios** (high demand 30%, low demand 70%)\n",
    "- **2 weeks** at hourly resolution (336 timesteps)\n",
    "\n",
    "This represents a capacity expansion problem where we optimize component sizes once,\n",
    "but operations are simulated across multiple future years and demand scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.generate_example_systems import create_multiperiod_system\n",
    "\n",
    "flow_system = create_multiperiod_system()\n",
    "\n",
    "print(f'Timesteps: {len(flow_system.timesteps)} ({len(flow_system.timesteps) // 24} days)')\n",
    "print(f'Periods: {list(flow_system.periods.values)}')\n",
    "print(f'Scenarios: {list(flow_system.scenarios.values)}')\n",
    "print(f'Scenario weights: {flow_system.scenario_weights.values}')\n",
    "print(f'\\nComponents: {list(flow_system.components.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Selecting a Subset with `transform.isel()`\n",
    "\n",
    "For demonstration purposes, we'll use only the first week of data.\n",
    "The `isel()` method (index select) lets you slice FlowSystems by time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first week only (168 hours)\n",
    "flow_system = flow_system.transform.isel(time=slice(0, 168))\n",
    "\n",
    "print(f'After isel: {len(flow_system.timesteps)} timesteps ({len(flow_system.timesteps) // 24} days)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize demand scenarios\n",
    "heat_demand = flow_system.components['Building'].inputs[0].fixed_relative_profile\n",
    "\n",
    "fig = px.line(\n",
    "    heat_demand.to_dataframe('value').reset_index(), x='time', y='value', facet_col='period', facet_row='scenario'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=350,\n",
    "    title='Heat Demand by Scenario (One Week)',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Heat Demand [kW]',\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Full Optimization (Baseline)\n",
    "\n",
    "First, solve the complete problem with all timesteps across all periods and scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = fx.solvers.HighsSolver(mip_gap=0.01)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "fs_full = flow_system.copy()\n",
    "fs_full.name = 'Full Optimization'\n",
    "fs_full.optimize(solver)\n",
    "time_full = timeit.default_timer() - start\n",
    "\n",
    "print(f'Full optimization: {time_full:.2f} seconds')\n",
    "print(f'Total cost (objective): {fs_full.solution[\"objective\"].item():,.0f} €')\n",
    "print('\\nOptimized sizes:')\n",
    "for name, size in fs_full.statistics.sizes.items():\n",
    "    print(f'  {name}: {size.max().item():.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Multi-Period Clustering with `cluster()`\n",
    "\n",
    "When applied to a multi-period system, `cluster()` clusters **each period/scenario combination independently**.\n",
    "This is because demand patterns and optimal operations may differ across:\n",
    "\n",
    "- **Periods**: Different years may have different characteristics\n",
    "- **Scenarios**: High vs low demand scenarios need different representative days\n",
    "\n",
    "The investment decisions (sizes) remain consistent across all periods and scenarios,\n",
    "while the operational patterns are optimized for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "# Force inclusion of peak demand periods\n",
    "peak_series = ['Building(Heat)|fixed_relative_profile']\n",
    "\n",
    "# Cluster to 3 typical days (from 7 days)\n",
    "fs_clustered = flow_system.transform.cluster(\n",
    "    n_clusters=3,\n",
    "    cluster_duration='1D',\n",
    "    time_series_for_high_peaks=peak_series,\n",
    ")\n",
    "\n",
    "time_clustering = timeit.default_timer() - start\n",
    "\n",
    "print(f'Clustering time: {time_clustering:.2f} seconds')\n",
    "print(f'Reduced: {len(flow_system.timesteps)} → {len(fs_clustered.timesteps)} timesteps per period')\n",
    "print('Total problem reduction: 7 days × 3 periods × 2 scenarios → 3 days × 3 × 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the reduced system\n",
    "start = timeit.default_timer()\n",
    "fs_clustered.optimize(solver)\n",
    "time_clustered = timeit.default_timer() - start\n",
    "\n",
    "print(f'Clustered optimization: {time_clustered:.2f} seconds')\n",
    "print(f'Total cost (objective): {fs_clustered.solution[\"objective\"].item():,.0f} €')\n",
    "print(f'\\nSpeedup vs full: {time_full / (time_clustering + time_clustered):.1f}x')\n",
    "print('\\nOptimized sizes:')\n",
    "for name, size in fs_clustered.statistics.sizes.items():\n",
    "    print(f'  {name}: {size.max().item():.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Visualize Clustering Quality\n",
    "\n",
    "The `.plot` accessor provides built-in visualizations with automatic faceting by period and scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs aggregated data - automatically faceted by period and scenario\n",
    "fs_clustered.clustering.plot.compare(variables='Building(Heat)|fixed_relative_profile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duration curves show how well the distribution is preserved per period/scenario\n",
    "fs_clustered.clustering.plot.compare(\n",
    "    kind='duration_curve',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap shows cluster assignments - faceted by period and scenario\n",
    "fs_clustered.clustering.plot.heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Understand the Cluster Structure\n",
    "\n",
    "Let's inspect how days were grouped into clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = fs_clustered.clustering\n",
    "cs = info.result.cluster_structure\n",
    "\n",
    "print('Clustering Configuration:')\n",
    "print(f'  Typical periods (clusters): {cs.n_clusters}')\n",
    "print(f'  Timesteps per cluster: {cs.timesteps_per_cluster}')\n",
    "\n",
    "# The cluster_order shows which cluster each original day belongs to\n",
    "cluster_order = cs.cluster_order.values\n",
    "day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "print('\\nCluster assignments per day:')\n",
    "for i, cluster_id in enumerate(cluster_order):\n",
    "    print(f'  {day_names[i]}: Cluster {cluster_id}')\n",
    "\n",
    "# Cluster occurrences (how many original days each cluster represents)\n",
    "unique, counts = np.unique(cluster_order, return_counts=True)\n",
    "print('\\nCluster weights (days represented):')\n",
    "for cluster_id, count in zip(unique, counts, strict=True):\n",
    "    print(f'  Cluster {cluster_id}: {count} days')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Two-Stage Workflow for Multi-Period\n",
    "\n",
    "For investment optimization across multiple periods, the recommended workflow is:\n",
    "\n",
    "1. **Stage 1**: Fast sizing with clustering (reduced timesteps)\n",
    "2. **Stage 2**: Fix sizes and run full-resolution dispatch\n",
    "\n",
    "This gives accurate investment decisions while maintaining computational tractability.\n",
    "\n",
    "### Safety Margin Rationale\n",
    "\n",
    "A 10% safety margin is applied to compensate for:\n",
    "\n",
    "- **Peak underestimation**: Clustering averages similar days, potentially underestimating true peak demands\n",
    "- **Temporal detail loss**: Representative periods may miss short-duration extreme events\n",
    "- **Scenario averaging**: Weighted scenarios smooth out worst-case conditions\n",
    "\n",
    "For critical applications, consider 15-20% margins or validate with full-resolution runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1 already done - apply safety margin\n",
    "SAFETY_MARGIN = 1.10  # 10% buffer for multi-period uncertainty\n",
    "\n",
    "sizes_with_margin = {name: size.max().item() * SAFETY_MARGIN for name, size in fs_clustered.statistics.sizes.items()}\n",
    "\n",
    "print('Stage 1: Sizing with clustering')\n",
    "print(f'  Time: {time_clustering + time_clustered:.2f} seconds')\n",
    "print(f'  Cost estimate: {fs_clustered.solution[\"objective\"].item():,.0f} €')\n",
    "print(f'\\nSizes with {(SAFETY_MARGIN - 1) * 100:.0f}% safety margin:')\n",
    "for name, size in sizes_with_margin.items():\n",
    "    original = fs_clustered.statistics.sizes[name].max().item()\n",
    "    print(f'  {name}: {original:.1f} → {size:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Full resolution dispatch with fixed sizes\n",
    "print('Stage 2: Full resolution dispatch')\n",
    "start = timeit.default_timer()\n",
    "\n",
    "fs_dispatch = flow_system.transform.fix_sizes(sizes_with_margin)\n",
    "fs_dispatch.name = 'Two-Stage'\n",
    "fs_dispatch.optimize(solver)\n",
    "\n",
    "time_dispatch = timeit.default_timer() - start\n",
    "\n",
    "print(f'  Time: {time_dispatch:.2f} seconds')\n",
    "print(f'  Actual cost: {fs_dispatch.solution[\"objective\"].item():,.0f} €')\n",
    "\n",
    "# Total comparison\n",
    "total_two_stage = time_clustering + time_clustered + time_dispatch\n",
    "print(f'\\nTotal two-stage time: {total_two_stage:.2f} seconds')\n",
    "print(f'Speedup vs full: {time_full / total_two_stage:.1f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Compare Results Across Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'Full (baseline)': {\n",
    "        'Time [s]': time_full,\n",
    "        'Cost [€]': fs_full.solution['objective'].item(),\n",
    "        'Boiler': fs_full.statistics.sizes['Boiler(Heat)'].max().item(),\n",
    "        'Storage': fs_full.statistics.sizes['ThermalStorage'].max().item(),\n",
    "    },\n",
    "    'Clustered (3 days)': {\n",
    "        'Time [s]': time_clustering + time_clustered,\n",
    "        'Cost [€]': fs_clustered.solution['objective'].item(),\n",
    "        'Boiler': fs_clustered.statistics.sizes['Boiler(Heat)'].max().item(),\n",
    "        'Storage': fs_clustered.statistics.sizes['ThermalStorage'].max().item(),\n",
    "    },\n",
    "    'Two-Stage': {\n",
    "        'Time [s]': total_two_stage,\n",
    "        'Cost [€]': fs_dispatch.solution['objective'].item(),\n",
    "        'Boiler': sizes_with_margin['Boiler(Heat)'],\n",
    "        'Storage': sizes_with_margin['ThermalStorage'],\n",
    "    },\n",
    "}\n",
    "\n",
    "comparison = pd.DataFrame(results).T\n",
    "baseline_cost = comparison.loc['Full (baseline)', 'Cost [€]']\n",
    "baseline_time = comparison.loc['Full (baseline)', 'Time [s]']\n",
    "comparison['Cost Gap [%]'] = ((comparison['Cost [€]'] - baseline_cost) / abs(baseline_cost) * 100).round(2)\n",
    "comparison['Speedup'] = (baseline_time / comparison['Time [s]']).round(1)\n",
    "\n",
    "comparison.style.format(\n",
    "    {\n",
    "        'Time [s]': '{:.2f}',\n",
    "        'Cost [€]': '{:,.0f}',\n",
    "        'Boiler': '{:.1f}',\n",
    "        'Storage': '{:.0f}',\n",
    "        'Cost Gap [%]': '{:.2f}',\n",
    "        'Speedup': '{:.1f}x',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Visualize Optimization Results\n",
    "\n",
    "Use the built-in statistics plotting to compare results across periods and scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot flow rates with automatic faceting by period and scenario\n",
    "fs_full.statistics.plot.flows(component='Boiler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison using the Comparison class\n",
    "comp = fx.Comparison([fs_full, fs_dispatch])\n",
    "comp.statistics.plot.balance('Heat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Expand Clustered Solution to Full Resolution\n",
    "\n",
    "Use `expand()` to map the clustered results back to all original timesteps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand the clustered solution\n",
    "fs_expanded = fs_clustered.transform.expand()\n",
    "\n",
    "print(f'Expanded: {len(fs_clustered.timesteps)} → {len(fs_expanded.timesteps)} timesteps')\n",
    "print(f'Cost (objective): {fs_expanded.solution[\"objective\"].item():,.0f} €')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare expanded solution - shows the repeated cluster patterns\n",
    "fs_expanded.statistics.plot.flows(component='Boiler')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Key Considerations for Multi-Period Clustering\n",
    "\n",
    "### 1. Independent Clustering per Period/Scenario\n",
    "\n",
    "Each period and scenario combination is clustered independently because:\n",
    "- Demand patterns may differ across years (growth, seasonality)\n",
    "- Scenarios represent different futures that shouldn't be mixed\n",
    "- Investment decisions must be robust across all combinations\n",
    "\n",
    "### 2. Safety Margins\n",
    "\n",
    "Multi-period systems often warrant larger safety margins (10-15%) because:\n",
    "- More uncertainty across multiple years\n",
    "- Investments made once must work for all periods\n",
    "- Scenario weights may not perfectly represent actual outcomes\n",
    "\n",
    "### 3. Computational Benefits\n",
    "\n",
    "Clustering becomes more valuable as problem size grows:\n",
    "\n",
    "| Scenario | Full Problem | With Clustering |\n",
    "|----------|--------------|----------------|\n",
    "| 1 period, 1 scenario, 365 days | 8,760 timesteps | ~730 (10 typical days) |\n",
    "| 3 periods, 2 scenarios, 365 days | 52,560 timesteps | ~4,380 |\n",
    "| 10 periods, 3 scenarios, 365 days | 262,800 timesteps | ~21,900 |\n",
    "\n",
    "The speedup factor increases with problem size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You learned how to:\n",
    "\n",
    "- Load **multi-period systems** with periods and scenarios\n",
    "- Use **`transform.isel()`** to select time subsets\n",
    "- Apply **`cluster()`** to multi-dimensional FlowSystems\n",
    "- **Visualize clustering** with the `.plot` accessor (compare, duration curves, heatmaps)\n",
    "- Use the **two-stage workflow** for robust investment optimization\n",
    "- **Expand solutions** back to full resolution with `expand()`\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Clustering is applied per period/scenario**: Each combination gets independent typical periods\n",
    "2. **Investments are shared**: Component sizes are optimized once across all periods/scenarios\n",
    "3. **Use larger safety margins**: Multi-period uncertainty warrants 10-15% buffers\n",
    "4. **Two-stage is recommended**: Fast sizing with clustering, accurate dispatch at full resolution\n",
    "5. **Built-in plotting**: Use `.plot` accessor for automatic faceting by period/scenario\n",
    "\n",
    "### API Reference\n",
    "\n",
    "```python\n",
    "# Load multi-period system\n",
    "fs = fx.FlowSystem.from_netcdf('multiperiod_system.nc4')\n",
    "\n",
    "# Select time subset (optional)\n",
    "fs = fs.transform.isel(time=slice(0, 168))  # First 168 timesteps\n",
    "\n",
    "# Cluster (applies per period/scenario)\n",
    "fs_clustered = fs.transform.cluster(\n",
    "    n_clusters=10,\n",
    "    cluster_duration='1D',\n",
    "    time_series_for_high_peaks=['Demand(Flow)|fixed_relative_profile'],\n",
    ")\n",
    "\n",
    "# Visualize clustering quality\n",
    "fs_clustered.clustering.plot.compare(variable='Demand(Flow)|profile')\n",
    "fs_clustered.clustering.plot.heatmap()\n",
    "\n",
    "# Two-stage workflow\n",
    "fs_clustered.optimize(solver)\n",
    "sizes = {k: v.max().item() * 1.10 for k, v in fs_clustered.statistics.sizes.items()}\n",
    "fs_dispatch = fs.transform.fix_sizes(sizes)\n",
    "fs_dispatch.optimize(solver)\n",
    "\n",
    "# Visualize results\n",
    "fs_dispatch.statistics.plot.flows(component='Boiler')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
