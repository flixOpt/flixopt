{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typical Periods Optimization with `cluster_reduce()`\n",
    "\n",
    "This notebook demonstrates the `cluster_reduce()` method for fast sizing optimization using typical periods.\n",
    "\n",
    "## Key Concept\n",
    "\n",
    "Unlike `cluster()` which uses equality constraints (same number of timesteps), `cluster_reduce()` **actually reduces** the number of timesteps:\n",
    "\n",
    "| Method | Timesteps | Mechanism | Use Case |\n",
    "|--------|-----------|-----------|----------|\n",
    "| `cluster()` | 2976 | Equality constraints | Accurate operational dispatch |\n",
    "| `cluster_reduce()` | 768 (8×96) | Typical periods only | Fast initial sizing |\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Actual timestep reduction**: Only solves for typical periods (e.g., 8 days × 96 timesteps = 768 instead of 2976)\n",
    "- **Timestep weighting**: Operational costs are weighted by cluster occurrence\n",
    "- **Inter-period storage linking**: SOC_boundary variables track storage state across original periods\n",
    "- **Cyclic constraint**: Optional cyclic storage constraint for long-term balance\n",
    "\n",
    "!!! note \"Requirements\"\n",
    "    This notebook requires the `tsam` package: `pip install tsam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import flixopt as fx\n",
    "\n",
    "fx.CONFIG.notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the FlowSystem\n",
    "\n",
    "We use a pre-built district heating system with real-world time series data (one month at 15-min resolution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Generate example data if not present (for local development)\n",
    "data_file = Path('data/district_heating_system.nc4')\n",
    "if not data_file.exists():\n",
    "    from data.generate_example_systems import create_district_heating_system\n",
    "\n",
    "    fs = create_district_heating_system()\n",
    "    fs.optimize(fx.solvers.HighsSolver(log_to_console=False))\n",
    "    fs.to_netcdf(data_file, overwrite=True)\n",
    "\n",
    "# Load the district heating system (real data from Zeitreihen2020.csv)\n",
    "flow_system = fx.FlowSystem.from_netcdf(data_file)\n",
    "\n",
    "timesteps = flow_system.timesteps\n",
    "print(f'Loaded FlowSystem: {len(timesteps)} timesteps ({len(timesteps) / 96:.0f} days at 15-min resolution)')\n",
    "print(f'Components: {list(flow_system.components.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first two weeks of data\n",
    "heat_demand = flow_system.components['HeatDemand'].inputs[0].fixed_relative_profile\n",
    "electricity_price = flow_system.components['GridBuy'].outputs[0].effects_per_flow_hour['costs']\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=timesteps[:1344], y=heat_demand.values[:1344], name='Heat Demand'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=timesteps[:1344], y=electricity_price.values[:1344], name='Electricity Price'), row=2, col=1)\n",
    "\n",
    "fig.update_layout(height=400, title='First Two Weeks of Data')\n",
    "fig.update_yaxes(title_text='Heat Demand [MW]', row=1, col=1)\n",
    "fig.update_yaxes(title_text='El. Price [€/MWh]', row=2, col=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Full Optimization (Baseline)\n",
    "\n",
    "First, let's solve the full problem with all timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = fx.solvers.HighsSolver(mip_gap=0.01)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "fs_full = flow_system.copy()\n",
    "fs_full.optimize(solver)\n",
    "time_full = timeit.default_timer() - start\n",
    "\n",
    "print(f'Full optimization: {time_full:.2f} seconds')\n",
    "print(f'Total cost: {fs_full.solution[\"costs\"].item():,.0f} €')\n",
    "print('\\nOptimized sizes:')\n",
    "for name, size in fs_full.statistics.sizes.items():\n",
    "    print(f'  {name}: {float(size.item()):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Typical Periods with `cluster_reduce()`\n",
    "\n",
    "Now let's use the `cluster_reduce()` method to solve with only 8 typical days (768 timesteps).\n",
    "\n",
    "**Important**: Use `time_series_for_high_peaks` to force inclusion of peak demand periods. Without this, the typical periods may miss extreme peaks, leading to undersized components that cause infeasibility in the full-resolution dispatch stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "# IMPORTANT: Use time_series_for_high_peaks to force inclusion of peak demand periods!\n",
    "# Without this, the typical periods may miss extreme peaks, leading to undersized components.\n",
    "# The format is the column name in the internal dataframe: 'ComponentName(FlowName)|attribute'\n",
    "peak_forcing_series = ['HeatDemand(Q_th)|fixed_relative_profile']\n",
    "\n",
    "# Create reduced FlowSystem with 8 typical days\n",
    "fs_reduced = flow_system.transform.cluster_reduce(\n",
    "    n_clusters=8,  # 8 typical days\n",
    "    cluster_duration='1D',  # Daily periods (can also use hours, e.g., 24)\n",
    "    time_series_for_high_peaks=peak_forcing_series,  # Force inclusion of peak demand day!\n",
    "    storage_inter_period_linking=True,  # Link storage states between periods\n",
    "    storage_cyclic=True,  # Cyclic constraint: SOC[0] = SOC[end]\n",
    ")\n",
    "\n",
    "time_clustering = timeit.default_timer() - start\n",
    "print(f'Clustering time: {time_clustering:.2f} seconds')\n",
    "print(f'Reduced from {len(flow_system.timesteps)} to {len(fs_reduced.timesteps)} timesteps')\n",
    "print(f'Timestep weights (cluster occurrences): {np.unique(fs_reduced._cluster_info[\"timestep_weights\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the reduced system\n",
    "start = timeit.default_timer()\n",
    "fs_reduced.optimize(solver)\n",
    "time_reduced = timeit.default_timer() - start\n",
    "\n",
    "print(f'Reduced optimization: {time_reduced:.2f} seconds')\n",
    "print(f'Total cost: {fs_reduced.solution[\"costs\"].item():,.0f} €')\n",
    "print(f'Speedup vs full: {time_full / (time_clustering + time_reduced):.1f}x')\n",
    "print('\\nOptimized sizes:')\n",
    "for name, size in fs_reduced.statistics.sizes.items():\n",
    "    print(f'  {name}: {float(size.item()):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Two-Stage Workflow\n",
    "\n",
    "The recommended workflow:\n",
    "1. **Stage 1**: Fast sizing with `cluster_reduce()`\n",
    "2. **Stage 2**: Fix sizes (with safety margin) and re-optimize for accurate dispatch\n",
    "\n",
    "**Note**: Typical periods aggregate similar days, so individual days within a cluster may have higher demand than the typical day. Adding a 5-10% safety margin to sizes helps ensure feasibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Fast sizing (already done above)\n",
    "print('Stage 1: Sizing with typical periods')\n",
    "print(f'  Time: {time_clustering + time_reduced:.2f} seconds')\n",
    "print(f'  Cost estimate: {fs_reduced.solution[\"costs\"].item():,.0f} €')\n",
    "\n",
    "# Apply safety margin to sizes (5-10% buffer for demand variability)\n",
    "SAFETY_MARGIN = 1.05  # 5% buffer\n",
    "sizes_with_margin = {name: float(size.item()) * SAFETY_MARGIN for name, size in fs_reduced.statistics.sizes.items()}\n",
    "print(f'\\nSizes with {(SAFETY_MARGIN - 1) * 100:.0f}% safety margin:')\n",
    "for name, size in sizes_with_margin.items():\n",
    "    original = fs_reduced.statistics.sizes[name].item()\n",
    "    print(f'  {name}: {original:.1f} -> {size:.1f}')\n",
    "\n",
    "# Stage 2: Fix sizes and re-optimize at full resolution\n",
    "print('\\nStage 2: Dispatch at full resolution')\n",
    "start = timeit.default_timer()\n",
    "\n",
    "fs_dispatch = flow_system.transform.fix_sizes(sizes_with_margin)\n",
    "fs_dispatch.optimize(solver)\n",
    "\n",
    "time_dispatch = timeit.default_timer() - start\n",
    "print(f'  Time: {time_dispatch:.2f} seconds')\n",
    "print(f'  Actual cost: {fs_dispatch.solution[\"costs\"].item():,.0f} €')\n",
    "\n",
    "# Total time comparison\n",
    "total_two_stage = time_clustering + time_reduced + time_dispatch\n",
    "print(f'\\nTotal two-stage time: {total_two_stage:.2f} seconds')\n",
    "print(f'Full optimization time: {time_full:.2f} seconds')\n",
    "print(f'Two-stage speedup: {time_full / total_two_stage:.1f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'Full (baseline)': {\n",
    "        'Time [s]': time_full,\n",
    "        'Cost [€]': fs_full.solution['costs'].item(),\n",
    "        'CHP Size': fs_full.statistics.sizes['CHP(Q_th)'].item(),\n",
    "        'Boiler Size': fs_full.statistics.sizes['Boiler(Q_th)'].item(),\n",
    "        'Storage Size': fs_full.statistics.sizes['Storage'].item(),\n",
    "    },\n",
    "    'Typical Periods (sizing)': {\n",
    "        'Time [s]': time_clustering + time_reduced,\n",
    "        'Cost [€]': fs_reduced.solution['costs'].item(),\n",
    "        'CHP Size': fs_reduced.statistics.sizes['CHP(Q_th)'].item(),\n",
    "        'Boiler Size': fs_reduced.statistics.sizes['Boiler(Q_th)'].item(),\n",
    "        'Storage Size': fs_reduced.statistics.sizes['Storage'].item(),\n",
    "    },\n",
    "    'Two-Stage (with margin)': {\n",
    "        'Time [s]': total_two_stage,\n",
    "        'Cost [€]': fs_dispatch.solution['costs'].item(),\n",
    "        'CHP Size': sizes_with_margin['CHP(Q_th)'],\n",
    "        'Boiler Size': sizes_with_margin['Boiler(Q_th)'],\n",
    "        'Storage Size': sizes_with_margin['Storage'],\n",
    "    },\n",
    "}\n",
    "\n",
    "comparison = pd.DataFrame(results).T\n",
    "baseline_cost = comparison.loc['Full (baseline)', 'Cost [€]']\n",
    "baseline_time = comparison.loc['Full (baseline)', 'Time [s]']\n",
    "comparison['Cost Gap [%]'] = ((comparison['Cost [€]'] - baseline_cost) / abs(baseline_cost) * 100).round(2)\n",
    "comparison['Speedup'] = (baseline_time / comparison['Time [s]']).round(1)\n",
    "\n",
    "comparison.style.format(\n",
    "    {\n",
    "        'Time [s]': '{:.2f}',\n",
    "        'Cost [€]': '{:,.0f}',\n",
    "        'CHP Size': '{:.1f}',\n",
    "        'Boiler Size': '{:.1f}',\n",
    "        'Storage Size': '{:.0f}',\n",
    "        'Cost Gap [%]': '{:.2f}',\n",
    "        'Speedup': '{:.1f}x',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-Period Storage Linking\n",
    "\n",
    "The `cluster_reduce()` method creates special constraints to track storage state across original periods:\n",
    "\n",
    "- **SOC_boundary[d]**: Storage state at the boundary of original period d\n",
    "- **delta_SOC[c]**: Change in SOC during typical period c\n",
    "- **Linking**: `SOC_boundary[d+1] = SOC_boundary[d] + delta_SOC[cluster_order[d]]`\n",
    "- **Cyclic**: `SOC_boundary[0] = SOC_boundary[end]` (optional)\n",
    "\n",
    "This ensures long-term storage behavior is captured correctly even though we only solve for typical periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show clustering info\n",
    "info = fs_reduced._cluster_info\n",
    "print('Typical Periods Configuration:')\n",
    "print(f'  Number of typical periods: {info[\"n_clusters\"]}')\n",
    "print(f'  Timesteps per period: {info[\"timesteps_per_cluster\"]}')\n",
    "print(f'  Total reduced timesteps: {info[\"n_clusters\"] * info[\"timesteps_per_cluster\"]}')\n",
    "print(f'  Cluster order (first 10): {info[\"cluster_order\"][:10]}...')\n",
    "cluster_occurrences = info['cluster_occurrences'][(None, None)]\n",
    "print(f'  Cluster occurrences: {dict(cluster_occurrences)}')\n",
    "print(f'  Storage inter-period linking: {info[\"storage_inter_period_linking\"]}')\n",
    "print(f'  Storage cyclic: {info[\"storage_cyclic\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Reference\n",
    "\n",
    "### `transform.cluster_reduce()` Parameters\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|-----------|------|-------------|\n",
    "| `n_clusters` | `int` | Number of typical periods to extract (e.g., 8) |\n",
    "| `cluster_duration` | `str \\| float` | Duration of each period ('1D', '24h') or hours as float |\n",
    "| `weights` | `dict[str, float]` | Optional weights for clustering each time series |\n",
    "| `time_series_for_high_peaks` | `list[str]` | **IMPORTANT**: Force inclusion of high-value periods to capture peak demands |\n",
    "| `time_series_for_low_peaks` | `list[str]` | Force inclusion of low-value periods |\n",
    "| `storage_inter_period_linking` | `bool` | Link storage states between periods (default: True) |\n",
    "| `storage_cyclic` | `bool` | Enforce cyclic storage constraint (default: True) |\n",
    "\n",
    "### Peak Forcing\n",
    "\n",
    "**Always use `time_series_for_high_peaks`** for demand time series to ensure extreme peaks are captured. The format is:\n",
    "```python\n",
    "time_series_for_high_peaks=['ComponentName(FlowName)|fixed_relative_profile']\n",
    "```\n",
    "\n",
    "Without peak forcing, the clustering algorithm may select typical periods that don't include the peak demand day, leading to undersized components and infeasibility in the dispatch stage.\n",
    "\n",
    "### Comparison with `cluster()`\n",
    "\n",
    "| Feature | `cluster()` | `cluster_reduce()` |\n",
    "|---------|-------------|--------------------|\n",
    "| Timesteps | Original (2976) | Reduced (e.g., 768) |\n",
    "| Mechanism | Equality constraints | Typical periods only |\n",
    "| Solve time | Moderate reduction | Dramatic reduction |\n",
    "| Accuracy | Higher | Lower (sizing only) |\n",
    "| Storage handling | Via constraints | SOC boundary linking |\n",
    "| Use case | Final dispatch | Initial sizing |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The `cluster_reduce()` method provides:\n",
    "\n",
    "1. **Dramatic speedup** for sizing optimization by reducing timesteps\n",
    "2. **Proper cost weighting** so operational costs reflect cluster occurrences\n",
    "3. **Storage state tracking** across original periods via SOC_boundary variables\n",
    "4. **Two-stage workflow** support via `fix_sizes()` for accurate dispatch\n",
    "\n",
    "### Recommended Workflow\n",
    "\n",
    "```python\n",
    "# Stage 1: Fast sizing with typical periods\n",
    "fs_sizing = flow_system.transform.cluster_reduce(\n",
    "    n_clusters=8,\n",
    "    cluster_duration='1D',\n",
    "    time_series_for_high_peaks=['DemandComponent(FlowName)|fixed_relative_profile'],\n",
    ")\n",
    "fs_sizing.optimize(solver)\n",
    "\n",
    "# Apply safety margin (typical periods aggregate, so individual days may exceed)\n",
    "SAFETY_MARGIN = 1.05  # 5% buffer\n",
    "sizes_with_margin = {\n",
    "    name: float(size.item()) * SAFETY_MARGIN\n",
    "    for name, size in fs_sizing.statistics.sizes.items()\n",
    "}\n",
    "\n",
    "# Stage 2: Fix sizes and optimize dispatch at full resolution\n",
    "fs_dispatch = flow_system.transform.fix_sizes(sizes_with_margin)\n",
    "fs_dispatch.optimize(solver)\n",
    "```\n",
    "\n",
    "### Key Considerations\n",
    "\n",
    "- **Peak forcing is essential**: Use `time_series_for_high_peaks` to capture peak demand days\n",
    "- **Safety margin recommended**: Add 5-10% buffer to sizes since aggregation smooths peaks\n",
    "- **Two-stage is recommended**: Use `cluster_reduce()` for fast sizing, then `fix_sizes()` for dispatch\n",
    "- **Storage linking preserves long-term behavior**: SOC_boundary variables ensure correct storage cycling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_expanded = fs_reduced.transform.expand_solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_expanded.statistics.plot.effects()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
