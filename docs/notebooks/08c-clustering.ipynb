{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Time Series Clustering with `cluster()`\n",
    "\n",
    "Accelerate investment optimization using typical periods (clustering).\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "- **Typical periods**: Cluster similar time segments (e.g., days) and solve only representative ones\n",
    "- **Weighted costs**: Automatically weight operational costs by cluster occurrence\n",
    "- **Two-stage workflow**: Fast sizing with clustering, accurate dispatch at full resolution\n",
    "\n",
    "!!! note \"Requirements\"\n",
    "    This notebook requires the `tsam` package: `pip install tsam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import flixopt as fx\n",
    "\n",
    "fx.CONFIG.notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Create the FlowSystem\n",
    "\n",
    "We use a district heating system with real-world time series data (one month at 15-min resolution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.generate_example_systems import create_district_heating_system\n",
    "\n",
    "flow_system = create_district_heating_system()\n",
    "flow_system.connect_and_transform()\n",
    "\n",
    "timesteps = flow_system.timesteps\n",
    "print(f'Loaded FlowSystem: {len(timesteps)} timesteps ({len(timesteps) / 24:.0f} days at hourly resolution)')\n",
    "print(f'Components: {list(flow_system.components.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize input data\n",
    "input_ds = xr.Dataset(\n",
    "    {\n",
    "        'Heat Demand': flow_system.components['HeatDemand'].inputs[0].fixed_relative_profile,\n",
    "        'Electricity Price': flow_system.components['GridBuy'].outputs[0].effects_per_flow_hour['costs'],\n",
    "    }\n",
    ")\n",
    "input_ds.fxplot.line(facet_row='variable', title='One Month of Input Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Method 1: Full Optimization (Baseline)\n",
    "\n",
    "First, solve the complete problem with all 2976 timesteps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = fx.solvers.HighsSolver(mip_gap=0.01)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "fs_full = flow_system.copy()\n",
    "fs_full.optimize(solver)\n",
    "time_full = timeit.default_timer() - start\n",
    "\n",
    "print(f'Full optimization: {time_full:.1f} seconds')\n",
    "print(f'Total cost: {fs_full.solution[\"costs\"].item():,.0f} €')\n",
    "print('\\nOptimized sizes:')\n",
    "for name, size in fs_full.statistics.sizes.items():\n",
    "    print(f'  {name}: {float(size.item()):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Method 2: Clustering with `cluster()`\n",
    "\n",
    "The `cluster()` method:\n",
    "\n",
    "1. **Clusters similar days** using the TSAM (Time Series Aggregation Module) package\n",
    "2. **Reduces timesteps** to only typical periods (e.g., 8 typical days = 768 timesteps)\n",
    "3. **Weights costs** by how many original days each typical day represents\n",
    "4. **Handles storage** with configurable behavior via `storage_mode`\n",
    "\n",
    "!!! warning \"Peak Forcing\"\n",
    "    Always use `time_series_for_high_peaks` to ensure extreme demand days are captured.\n",
    "    Without this, clustering may miss peak periods, causing undersized components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "# IMPORTANT: Force inclusion of peak demand periods!\n",
    "peak_series = ['HeatDemand(Q_th)|fixed_relative_profile']\n",
    "\n",
    "# Create reduced FlowSystem with 8 typical days\n",
    "fs_clustered = flow_system.transform.cluster(\n",
    "    n_clusters=8,  # 8 typical days\n",
    "    cluster_duration='1D',  # Daily clustering\n",
    "    time_series_for_high_peaks=peak_series,  # Capture peak demand day\n",
    "    random_state=42,  # Reproducible results\n",
    ")\n",
    "\n",
    "time_clustering = timeit.default_timer() - start\n",
    "\n",
    "print(\n",
    "    f'Clustering: {len(flow_system.timesteps)} → {len(fs_clustered.timesteps) * len(fs_clustered.clusters)} timesteps'\n",
    ")\n",
    "print(f'  Clusters: {len(fs_clustered.clusters)}')\n",
    "print(f'  Time: {time_clustering:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the reduced system\n",
    "start = timeit.default_timer()\n",
    "fs_clustered.optimize(solver)\n",
    "time_clustered = timeit.default_timer() - start\n",
    "\n",
    "print(f'Clustered optimization: {time_clustered:.1f} seconds')\n",
    "print(f'Total cost: {fs_clustered.solution[\"costs\"].item():,.0f} €')\n",
    "print(f'\\nSpeedup vs full: {time_full / (time_clustering + time_clustered):.1f}x')\n",
    "print('\\nOptimized sizes:')\n",
    "for name, size in fs_clustered.statistics.sizes.items():\n",
    "    print(f'  {name}: {float(size.item()):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Understanding the Clustering\n",
    "\n",
    "The clustering algorithm groups similar days together. Access all metadata via `fs.clustering`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access clustering metadata directly\n",
    "clustering = fs_clustered.clustering\n",
    "clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key properties\n",
    "print(f'Clusters: {clustering.n_clusters}')\n",
    "print(f'Original segments (days): {clustering.n_original_clusters}')\n",
    "print(f'Timesteps per cluster: {clustering.timesteps_per_cluster}')\n",
    "print(f'\\nCluster occurrences: {clustering.occurrences.values}')\n",
    "print(f'Cluster order: {clustering.cluster_order.values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality metrics - how well do the clusters represent the original data?\n",
    "# Lower RMSE/MAE = better representation\n",
    "clustering.metrics.to_dataframe().style.format('{:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison: original vs clustered time series\n",
    "clustering.plot.compare()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Advanced Clustering Options\n",
    "\n",
    "The `cluster()` method exposes many parameters for fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different clustering algorithms\n",
    "fs_hierarchical = flow_system.transform.cluster(\n",
    "    n_clusters=8,\n",
    "    cluster_duration='1D',\n",
    "    cluster_method='hierarchical',  # Alternative: 'k_means' (default), 'k_medoids', 'averaging'\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Compare cluster assignments between algorithms\n",
    "print('k_means clusters:    ', fs_clustered.clustering.cluster_order.values)\n",
    "print('hierarchical clusters:', fs_hierarchical.clustering.cluster_order.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare RMSE between algorithms\n",
    "print('Quality comparison (RMSE for HeatDemand):')\n",
    "print(\n",
    "    f'  k_means:      {float(fs_clustered.clustering.metrics[\"RMSE\"].sel(time_series=\"HeatDemand(Q_th)|fixed_relative_profile\")):.4f}'\n",
    ")\n",
    "print(\n",
    "    f'  hierarchical: {float(fs_hierarchical.clustering.metrics[\"RMSE\"].sel(time_series=\"HeatDemand(Q_th)|fixed_relative_profile\")):.4f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster structure with heatmap\n",
    "clustering.plot.heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Method 3: Two-Stage Workflow (Recommended)\n",
    "\n",
    "The recommended approach for investment optimization:\n",
    "\n",
    "1. **Stage 1**: Fast sizing with `cluster()` \n",
    "2. **Stage 2**: Fix sizes (with safety margin) and dispatch at full resolution\n",
    "\n",
    "!!! tip \"Safety Margin\"\n",
    "    Typical periods aggregate similar days, so individual days may have higher demand \n",
    "    than the typical day. Adding a 5-10% margin ensures feasibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1 already done above\n",
    "print('Stage 1: Sizing with typical periods')\n",
    "print(f'  Time: {time_clustering + time_clustered:.1f} seconds')\n",
    "print(f'  Cost estimate: {fs_clustered.solution[\"costs\"].item():,.0f} €')\n",
    "\n",
    "# Apply safety margin to sizes\n",
    "SAFETY_MARGIN = 1.05  # 5% buffer\n",
    "sizes_with_margin = {name: float(size.item()) * SAFETY_MARGIN for name, size in fs_clustered.statistics.sizes.items()}\n",
    "print(f'\\nSizes with {(SAFETY_MARGIN - 1) * 100:.0f}% safety margin:')\n",
    "for name, size in sizes_with_margin.items():\n",
    "    original = fs_clustered.statistics.sizes[name].item()\n",
    "    print(f'  {name}: {original:.1f} → {size:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Fix sizes and optimize at full resolution\n",
    "print('Stage 2: Dispatch at full resolution')\n",
    "start = timeit.default_timer()\n",
    "\n",
    "fs_dispatch = flow_system.transform.fix_sizes(sizes_with_margin)\n",
    "fs_dispatch.optimize(solver)\n",
    "\n",
    "time_dispatch = timeit.default_timer() - start\n",
    "print(f'  Time: {time_dispatch:.1f} seconds')\n",
    "print(f'  Actual cost: {fs_dispatch.solution[\"costs\"].item():,.0f} €')\n",
    "\n",
    "# Total comparison\n",
    "total_two_stage = time_clustering + time_clustered + time_dispatch\n",
    "print(f'\\nTotal two-stage time: {total_two_stage:.1f} seconds')\n",
    "print(f'Speedup vs full: {time_full / total_two_stage:.1f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'Full (baseline)': {\n",
    "        'Time [s]': time_full,\n",
    "        'Cost [€]': fs_full.solution['costs'].item(),\n",
    "        'CHP': fs_full.statistics.sizes['CHP(Q_th)'].item(),\n",
    "        'Boiler': fs_full.statistics.sizes['Boiler(Q_th)'].item(),\n",
    "        'Storage': fs_full.statistics.sizes['Storage'].item(),\n",
    "    },\n",
    "    'Clustered (8 days)': {\n",
    "        'Time [s]': time_clustering + time_clustered,\n",
    "        'Cost [€]': fs_clustered.solution['costs'].item(),\n",
    "        'CHP': fs_clustered.statistics.sizes['CHP(Q_th)'].item(),\n",
    "        'Boiler': fs_clustered.statistics.sizes['Boiler(Q_th)'].item(),\n",
    "        'Storage': fs_clustered.statistics.sizes['Storage'].item(),\n",
    "    },\n",
    "    'Two-Stage': {\n",
    "        'Time [s]': total_two_stage,\n",
    "        'Cost [€]': fs_dispatch.solution['costs'].item(),\n",
    "        'CHP': sizes_with_margin['CHP(Q_th)'],\n",
    "        'Boiler': sizes_with_margin['Boiler(Q_th)'],\n",
    "        'Storage': sizes_with_margin['Storage'],\n",
    "    },\n",
    "}\n",
    "\n",
    "comparison = pd.DataFrame(results).T\n",
    "baseline_cost = comparison.loc['Full (baseline)', 'Cost [€]']\n",
    "baseline_time = comparison.loc['Full (baseline)', 'Time [s]']\n",
    "comparison['Cost Gap [%]'] = ((comparison['Cost [€]'] - baseline_cost) / abs(baseline_cost) * 100).round(2)\n",
    "comparison['Speedup'] = (baseline_time / comparison['Time [s]']).round(1)\n",
    "\n",
    "comparison.style.format(\n",
    "    {\n",
    "        'Time [s]': '{:.1f}',\n",
    "        'Cost [€]': '{:,.0f}',\n",
    "        'CHP': '{:.1f}',\n",
    "        'Boiler': '{:.1f}',\n",
    "        'Storage': '{:.0f}',\n",
    "        'Cost Gap [%]': '{:.2f}',\n",
    "        'Speedup': '{:.1f}x',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Expand Solution to Full Resolution\n",
    "\n",
    "Use `expand_solution()` to map the clustered solution back to all original timesteps.\n",
    "This repeats the typical period values for all days belonging to that cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand the clustered solution to full resolution\n",
    "fs_expanded = fs_clustered.transform.expand_solution()\n",
    "\n",
    "print(f'Expanded: {len(fs_clustered.timesteps)} → {len(fs_expanded.timesteps)} timesteps')\n",
    "print(f'Cost: {fs_expanded.solution[\"costs\"].item():,.0f} €')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare heat production: Full vs Expanded\n",
    "heat_flows = ['CHP(Q_th)|flow_rate', 'Boiler(Q_th)|flow_rate']\n",
    "\n",
    "# Create comparison dataset\n",
    "comparison_ds = xr.Dataset(\n",
    "    {\n",
    "        name.replace('|flow_rate', ''): xr.concat(\n",
    "            [fs_full.solution[name], fs_expanded.solution[name]], dim=pd.Index(['Full', 'Expanded'], name='method')\n",
    "        )\n",
    "        for name in heat_flows\n",
    "    }\n",
    ")\n",
    "\n",
    "comparison_ds.fxplot.line(facet_col='variable', color='method', title='Heat Production Comparison')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Visualize Clustered Heat Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_clustered.statistics.plot.storage('Storage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_expanded.statistics.plot.storage('Storage').data.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## API Reference\n",
    "\n",
    "### `transform.cluster()` Parameters\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| `n_clusters` | `int` | - | Number of typical periods (e.g., 8 typical days) |\n",
    "| `cluster_duration` | `str \\| float` | - | Duration per cluster ('1D', '24h') or hours |\n",
    "| `weights` | `dict[str, float]` | None | Optional weights for time series in clustering |\n",
    "| `time_series_for_high_peaks` | `list[str]` | None | **Essential**: Force inclusion of peak periods |\n",
    "| `time_series_for_low_peaks` | `list[str]` | None | Force inclusion of minimum periods |\n",
    "| `cluster_method` | `str` | 'k_means' | Algorithm: 'k_means', 'hierarchical', 'k_medoids', 'k_maxoids', 'averaging' |\n",
    "| `representation_method` | `str` | 'meanRepresentation' | 'meanRepresentation', 'medoidRepresentation', 'distributionAndMinMaxRepresentation' |\n",
    "| `extreme_period_method` | `str` | 'new_cluster_center' | How peaks are integrated: 'None', 'append', 'new_cluster_center', 'replace_cluster_center' |\n",
    "| `rescale_cluster_periods` | `bool` | True | Rescale clusters to match original means |\n",
    "| `random_state` | `int` | None | Random seed for reproducibility |\n",
    "| `predef_cluster_order` | `array` | None | Manual cluster assignments |\n",
    "| `**tsam_kwargs` | - | - | Additional tsam parameters |\n",
    "\n",
    "### Clustering Object Properties\n",
    "\n",
    "After clustering, access metadata via `fs.clustering`:\n",
    "\n",
    "| Property | Description |\n",
    "|----------|-------------|\n",
    "| `n_clusters` | Number of clusters |\n",
    "| `n_original_clusters` | Number of original time segments (e.g., 365 days) |\n",
    "| `timesteps_per_cluster` | Timesteps in each cluster (e.g., 24 for daily) |\n",
    "| `cluster_order` | xr.DataArray mapping original segment → cluster ID |\n",
    "| `occurrences` | How many original segments each cluster represents |\n",
    "| `metrics` | xr.Dataset with RMSE, MAE per time series |\n",
    "| `plot.compare()` | Compare original vs clustered time series |\n",
    "| `plot.heatmap()` | Visualize cluster structure |\n",
    "\n",
    "### Storage Behavior\n",
    "\n",
    "Each `Storage` component has a `cluster_mode` parameter:\n",
    "\n",
    "| Mode | Description |\n",
    "|------|-------------|\n",
    "| `'intercluster_cyclic'` | Links storage across clusters + yearly cyclic **(default)** |\n",
    "| `'intercluster'` | Links storage across clusters, free start/end |\n",
    "| `'cyclic'` | Each cluster is independent but cyclic (start = end) |\n",
    "| `'independent'` | Each cluster is independent, free start/end |\n",
    "\n",
    "For a detailed comparison of storage modes, see [08c2-clustering-storage-modes](08c2-clustering-storage-modes.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You learned how to:\n",
    "\n",
    "- Use **`cluster()`** to reduce time series into typical periods\n",
    "- Apply **peak forcing** to capture extreme demand days\n",
    "- Use **two-stage optimization** for fast yet accurate investment decisions\n",
    "- **Expand solutions** back to full resolution with `expand_solution()`\n",
    "- Access **clustering metadata** via `fs.clustering` (metrics, cluster_order, occurrences)\n",
    "- Use **advanced options** like different algorithms and reproducible random states\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Always use peak forcing** (`time_series_for_high_peaks`) for demand time series\n",
    "2. **Add safety margin** (5-10%) when fixing sizes from clustering\n",
    "3. **Two-stage is recommended**: clustering for sizing, full resolution for dispatch\n",
    "4. **Storage handling** is configurable via `cluster_mode`\n",
    "5. **Use `random_state`** for reproducible results\n",
    "6. **Check metrics** to evaluate clustering quality\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **[08c2-clustering-storage-modes](08c2-clustering-storage-modes.ipynb)**: Compare storage modes using a seasonal storage system\n",
    "- **[08d-clustering-multiperiod](08d-clustering-multiperiod.ipynb)**: Clustering with multiple periods and scenarios"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
