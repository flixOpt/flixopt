{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Time Series Clustering with `cluster()`\n",
    "\n",
    "Accelerate investment optimization using typical periods (clustering).\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "- **Typical periods**: Cluster similar time segments (e.g., days) and solve only representative ones\n",
    "- **Weighted costs**: Automatically weight operational costs by cluster occurrence\n",
    "- **Two-stage workflow**: Fast sizing with clustering, accurate dispatch at full resolution\n",
    "\n",
    "!!! note \"Requirements\"\n",
    "    This notebook requires the `tsam` package with `ClusterConfig` and `ExtremeConfig` support.\n",
    "    Install with: `pip install \"flixopt[full]\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import flixopt as fx\n",
    "\n",
    "fx.CONFIG.notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Create the FlowSystem\n",
    "\n",
    "We use a district heating system with real-world time series data (one month at 15-min resolution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.generate_example_systems import create_district_heating_system\n",
    "\n",
    "flow_system = create_district_heating_system()\n",
    "flow_system.connect_and_transform()\n",
    "\n",
    "timesteps = flow_system.timesteps\n",
    "\n",
    "flow_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize input data\n",
    "input_ds = xr.Dataset(\n",
    "    {\n",
    "        'Heat Demand': flow_system.components['HeatDemand'].inputs[0].fixed_relative_profile,\n",
    "        'Electricity Price': flow_system.components['GridBuy'].outputs[0].effects_per_flow_hour['costs'],\n",
    "    }\n",
    ")\n",
    "input_ds.plotly.line(x='time', facet_row='variable', title='One Month of Input Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Method 1: Full Optimization (Baseline)\n",
    "\n",
    "First, solve the complete problem with all 2976 timesteps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = fx.solvers.HighsSolver(mip_gap=0.01)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "fs_full = flow_system.copy()\n",
    "fs_full.name = 'Full Optimization'\n",
    "fs_full.optimize(solver)\n",
    "time_full = timeit.default_timer() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Method 2: Clustering with `cluster()`\n",
    "\n",
    "The `cluster()` method:\n",
    "\n",
    "1. **Clusters similar days** using the TSAM (Time Series Aggregation Module) package\n",
    "2. **Reduces timesteps** to only typical periods (e.g., 8 typical days = 768 timesteps)\n",
    "3. **Weights costs** by how many original days each typical day represents\n",
    "4. **Handles storage** with configurable behavior via `storage_mode`\n",
    "\n",
    "!!! warning \"Peak Forcing\"\n",
    "    Always use `extremes=ExtremeConfig(max_value=[...])` to ensure extreme demand days are captured.\n",
    "    Without this, clustering may miss peak periods, causing undersized components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsam import ExtremeConfig\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# IMPORTANT: Force inclusion of peak demand periods!\n",
    "peak_series = ['HeatDemand(Q_th)|fixed_relative_profile']\n",
    "\n",
    "# Create reduced FlowSystem with 8 typical days\n",
    "fs_clustered = flow_system.transform.cluster(\n",
    "    n_clusters=8,  # 8 typical days\n",
    "    cluster_duration='1D',  # Daily clustering\n",
    "    extremes=ExtremeConfig(method='new_cluster', max_value=peak_series),  # Capture peak demand day\n",
    ")\n",
    "fs_clustered.name = 'Clustered (8 days)'\n",
    "\n",
    "time_clustering = timeit.default_timer() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the reduced system\n",
    "start = timeit.default_timer()\n",
    "fs_clustered.optimize(solver)\n",
    "time_clustered = timeit.default_timer() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Understanding the Clustering\n",
    "\n",
    "The clustering algorithm groups similar days together. Access all metadata via `fs.clustering`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access clustering metadata directly\n",
    "clustering = fs_clustered.clustering.results\n",
    "clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show clustering info using __repr__\n",
    "fs_clustered.clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality metrics - how well do the clusters represent the original data?\n",
    "# Lower RMSE/MAE = better representation\n",
    "fs_clustered.clustering.metrics.to_dataframe().style.format('{:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison: original vs clustered time series\n",
    "fs_clustered.clustering.plot.compare()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Inspect Clustering Input Data\n",
    "\n",
    "Before clustering, you can inspect which time-varying data will be used.\n",
    "The `clustering_data()` method returns only the arrays that vary over time\n",
    "(constant arrays are excluded since they don't affect clustering):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what data will be used for clustering\n",
    "clustering_data = flow_system.transform.clustering_data()\n",
    "print(f'Variables used for clustering ({len(clustering_data.data_vars)} total):')\n",
    "for var in clustering_data.data_vars:\n",
    "    print(f'  - {var}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the time-varying data (select a few key variables)\n",
    "key_vars = [v for v in clustering_data.data_vars if 'fixed_relative_profile' in v or 'effects_per_flow_hour' in v]\n",
    "clustering_data[key_vars].plotly.line(facet_row='variable', title='Time-Varying Data Used for Clustering')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Selective Clustering with `data_vars`\n",
    "\n",
    "By default, clustering uses **all** time-varying data to determine typical periods.\n",
    "However, you may want to cluster based on only a **subset** of variables while still\n",
    "applying the clustering to all data.\n",
    "\n",
    "Use the `data_vars` parameter to specify which variables determine the clustering:\n",
    "\n",
    "- **Cluster based on subset**: Only the specified variables affect which days are grouped together\n",
    "- **Apply to all data**: The resulting clustering is applied to ALL time-varying data\n",
    "\n",
    "This is useful when:\n",
    "- You want to cluster based on demand patterns only (ignoring price variations)\n",
    "- You have dominant time series that should drive the clustering\n",
    "- You want to ensure certain patterns are well-represented in typical periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster based ONLY on heat demand pattern (ignore electricity prices)\n",
    "demand_var = 'HeatDemand(Q_th)|fixed_relative_profile'\n",
    "\n",
    "fs_demand_only = flow_system.transform.cluster(\n",
    "    n_clusters=8,\n",
    "    cluster_duration='1D',\n",
    "    data_vars=[demand_var],  # Only this variable determines clustering\n",
    "    extremes=ExtremeConfig(method='new_cluster', max_value=[demand_var]),\n",
    ")\n",
    "\n",
    "# Verify: clustering was determined by demand but applied to all data\n",
    "print(f'Clustered using: {demand_var}')\n",
    "print(f'But all {len(clustering_data.data_vars)} variables are included in the result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare metrics: clustering with all data vs. demand-only\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        'All Variables': fs_clustered.clustering.metrics.to_dataframe().iloc[0],\n",
    "        'Demand Only': fs_demand_only.clustering.metrics.to_dataframe().iloc[0],\n",
    "    }\n",
    ").round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Advanced Clustering Options\n",
    "\n",
    "The `cluster()` method exposes many parameters for fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsam import ClusterConfig\n",
    "\n",
    "# Try different clustering algorithms\n",
    "fs_kmeans = flow_system.transform.cluster(\n",
    "    n_clusters=8,\n",
    "    cluster_duration='1D',\n",
    "    cluster=ClusterConfig(method='kmeans'),  # Alternative: 'hierarchical' (default), 'kmedoids', 'averaging'\n",
    ")\n",
    "\n",
    "fs_kmeans.clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare quality metrics between algorithms\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        'hierarchical': fs_clustered.clustering.metrics.to_dataframe().iloc[0],\n",
    "        'kmeans': fs_kmeans.clustering.metrics.to_dataframe().iloc[0],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster structure with heatmap\n",
    "fs_clustered.clustering.plot.heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Apply Existing Clustering\n",
    "\n",
    "When comparing design variants or performing sensitivity analysis, you often want to\n",
    "use the **same cluster structure** across different FlowSystem configurations.\n",
    "Use `apply_clustering()` to reuse a clustering from another FlowSystem:\n",
    "\n",
    "```python\n",
    "# First, create a reference clustering\n",
    "fs_reference = flow_system.transform.cluster(n_clusters=8, cluster_duration='1D')\n",
    "\n",
    "# Modify the FlowSystem (e.g., different storage size)\n",
    "flow_system_modified = flow_system.copy()\n",
    "flow_system_modified.components['Storage'].capacity_in_flow_hours.maximum_size = 2000\n",
    "\n",
    "# Apply the SAME clustering for fair comparison\n",
    "fs_modified = flow_system_modified.transform.apply_clustering(fs_reference.clustering)\n",
    "```\n",
    "\n",
    "This ensures both systems use identical typical periods for fair comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Method 3: Two-Stage Workflow (Recommended)\n",
    "\n",
    "The recommended approach for investment optimization:\n",
    "\n",
    "1. **Stage 1**: Fast sizing with `cluster()` \n",
    "2. **Stage 2**: Fix sizes (with safety margin) and dispatch at full resolution\n",
    "\n",
    "!!! tip \"Safety Margin\"\n",
    "    Typical periods aggregate similar days, so individual days may have higher demand \n",
    "    than the typical day. Adding a 5-10% margin ensures feasibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply safety margin to sizes\n",
    "SAFETY_MARGIN = 1.05  # 5% buffer\n",
    "sizes_with_margin = {name: float(size.item()) * SAFETY_MARGIN for name, size in fs_clustered.stats.sizes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Fix sizes and optimize at full resolution\n",
    "start = timeit.default_timer()\n",
    "\n",
    "fs_dispatch = flow_system.transform.fix_sizes(sizes_with_margin)\n",
    "fs_dispatch.name = 'Two-Stage'\n",
    "fs_dispatch.optimize(solver)\n",
    "\n",
    "time_dispatch = timeit.default_timer() - start\n",
    "\n",
    "# Total two-stage time\n",
    "total_two_stage = time_clustering + time_clustered + time_dispatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'Full (baseline)': {\n",
    "        'Time [s]': time_full,\n",
    "        'Cost [€]': fs_full.solution['costs'].item(),\n",
    "        'CHP': fs_full.stats.sizes['CHP(Q_th)'].item(),\n",
    "        'Boiler': fs_full.stats.sizes['Boiler(Q_th)'].item(),\n",
    "        'Storage': fs_full.stats.sizes['Storage'].item(),\n",
    "    },\n",
    "    'Clustered (8 days)': {\n",
    "        'Time [s]': time_clustering + time_clustered,\n",
    "        'Cost [€]': fs_clustered.solution['costs'].item(),\n",
    "        'CHP': fs_clustered.stats.sizes['CHP(Q_th)'].item(),\n",
    "        'Boiler': fs_clustered.stats.sizes['Boiler(Q_th)'].item(),\n",
    "        'Storage': fs_clustered.stats.sizes['Storage'].item(),\n",
    "    },\n",
    "    'Two-Stage': {\n",
    "        'Time [s]': total_two_stage,\n",
    "        'Cost [€]': fs_dispatch.solution['costs'].item(),\n",
    "        'CHP': sizes_with_margin['CHP(Q_th)'],\n",
    "        'Boiler': sizes_with_margin['Boiler(Q_th)'],\n",
    "        'Storage': sizes_with_margin['Storage'],\n",
    "    },\n",
    "}\n",
    "\n",
    "comparison = pd.DataFrame(results).T\n",
    "baseline_cost = comparison.loc['Full (baseline)', 'Cost [€]']\n",
    "baseline_time = comparison.loc['Full (baseline)', 'Time [s]']\n",
    "comparison['Cost Gap [%]'] = ((comparison['Cost [€]'] - baseline_cost) / abs(baseline_cost) * 100).round(2)\n",
    "comparison['Speedup'] = (baseline_time / comparison['Time [s]']).round(1)\n",
    "\n",
    "comparison.style.format(\n",
    "    {\n",
    "        'Time [s]': '{:.1f}',\n",
    "        'Cost [€]': '{:,.0f}',\n",
    "        'CHP': '{:.1f}',\n",
    "        'Boiler': '{:.1f}',\n",
    "        'Storage': '{:.0f}',\n",
    "        'Cost Gap [%]': '{:.2f}',\n",
    "        'Speedup': '{:.1f}x',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Expand Solution to Full Resolution\n",
    "\n",
    "Use `expand()` to map the clustered solution back to all original timesteps.\n",
    "This repeats the typical period values for all days belonging to that cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand the clustered solution to full resolution\n",
    "fs_expanded = fs_clustered.transform.expand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare heat production: Full vs Expanded\n",
    "heat_flows = ['CHP(Q_th)|flow_rate', 'Boiler(Q_th)|flow_rate']\n",
    "\n",
    "# Create comparison dataset\n",
    "comparison_ds = xr.Dataset(\n",
    "    {\n",
    "        name.replace('|flow_rate', ''): xr.concat(\n",
    "            [fs_full.solution[name], fs_expanded.solution[name]], dim=pd.Index(['Full', 'Expanded'], name='method')\n",
    "        )\n",
    "        for name in heat_flows\n",
    "    }\n",
    ")\n",
    "\n",
    "comparison_ds.plotly.line(x='time', facet_col='variable', color='method', title='Heat Production Comparison')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Visualize Clustered Heat Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_clustered.stats.plot.storage('Storage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_expanded.stats.plot.storage('Storage')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "<cell_type>markdown</cell_type>## API Reference\n",
    "\n",
    "### `transform.cluster()` Parameters\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| `n_clusters` | `int` | - | Number of typical periods (e.g., 8 typical days) |\n",
    "| `cluster_duration` | `str \\| float` | - | Duration per cluster ('1D', '24h') or hours |\n",
    "| `data_vars` | `list[str]` | None | Variables to cluster on (applies result to all) |\n",
    "| `weights` | `dict[str, float]` | None | Optional weights for time series in clustering |\n",
    "| `cluster` | `ClusterConfig` | None | Clustering algorithm configuration |\n",
    "| `extremes` | `ExtremeConfig` | None | **Essential**: Force inclusion of peak/min periods |\n",
    "| `**tsam_kwargs` | - | - | Additional tsam parameters |\n",
    "\n",
    "### `transform.clustering_data()` Method\n",
    "\n",
    "Inspect which time-varying data will be used for clustering:\n",
    "\n",
    "```python\n",
    "# Get all time-varying variables\n",
    "clustering_data = flow_system.transform.clustering_data()\n",
    "print(list(clustering_data.data_vars))\n",
    "\n",
    "# Get data for a specific period (multi-period systems)\n",
    "clustering_data = flow_system.transform.clustering_data(period=2024)\n",
    "```\n",
    "\n",
    "### Clustering Object Properties\n",
    "\n",
    "After clustering, access metadata via `fs.clustering`:\n",
    "\n",
    "| Property | Description |\n",
    "|----------|-------------|\n",
    "| `n_clusters` | Number of clusters |\n",
    "| `n_original_clusters` | Number of original time segments (e.g., 365 days) |\n",
    "| `timesteps_per_cluster` | Timesteps in each cluster (e.g., 24 for daily) |\n",
    "| `cluster_assignments` | xr.DataArray mapping original segment → cluster ID |\n",
    "| `cluster_occurrences` | How many original segments each cluster represents |\n",
    "| `metrics` | xr.Dataset with RMSE, MAE per time series |\n",
    "| `results` | `ClusteringResults` with xarray-like interface |\n",
    "| `plot.compare()` | Compare original vs clustered time series |\n",
    "| `plot.heatmap()` | Visualize cluster structure |\n",
    "\n",
    "### ClusteringResults (xarray-like)\n",
    "\n",
    "Access the underlying tsam results via `clustering.results`:\n",
    "\n",
    "```python\n",
    "# Dimension info (like xarray)\n",
    "clustering.results.dims      # ('period', 'scenario') or ()\n",
    "clustering.results.coords    # {'period': [2020, 2030], 'scenario': ['high', 'low']}\n",
    "\n",
    "# Select specific result (like xarray)\n",
    "clustering.results.sel(period=2020, scenario='high')   # Label-based\n",
    "clustering.results.isel(period=0, scenario=1)          # Index-based\n",
    "\n",
    "# Apply existing clustering to new data\n",
    "agg_results = clustering.results.apply(dataset)  # Returns AggregationResults\n",
    "```\n",
    "\n",
    "### Storage Behavior\n",
    "\n",
    "Each `Storage` component has a `cluster_mode` parameter:\n",
    "\n",
    "| Mode | Description |\n",
    "|------|-------------|\n",
    "| `'intercluster_cyclic'` | Links storage across clusters + yearly cyclic **(default)** |\n",
    "| `'intercluster'` | Links storage across clusters, free start/end |\n",
    "| `'cyclic'` | Each cluster is independent but cyclic (start = end) |\n",
    "| `'independent'` | Each cluster is independent, free start/end |\n",
    "\n",
    "For a detailed comparison of storage modes, see [08c2-clustering-storage-modes](08c2-clustering-storage-modes.ipynb).\n",
    "\n",
    "### Peak Forcing with ExtremeConfig\n",
    "\n",
    "```python\n",
    "from tsam import ExtremeConfig\n",
    "\n",
    "extremes = ExtremeConfig(\n",
    "    method='new_cluster',  # Creates new cluster for extremes\n",
    "    max_value=['ComponentName(FlowName)|fixed_relative_profile'],  # Capture peak demand\n",
    ")\n",
    "```\n",
    "\n",
    "### Recommended Workflow\n",
    "\n",
    "```python\n",
    "from tsam import ExtremeConfig\n",
    "\n",
    "# Stage 1: Fast sizing\n",
    "fs_sizing = flow_system.transform.cluster(\n",
    "    n_clusters=8,\n",
    "    cluster_duration='1D',\n",
    "    extremes=ExtremeConfig(method='new_cluster', max_value=['Demand(Flow)|fixed_relative_profile']),\n",
    ")\n",
    "fs_sizing.optimize(solver)\n",
    "\n",
    "# Apply safety margin\n",
    "sizes = {k: v.item() * 1.05 for k, v in fs_sizing.stats.sizes.items()}\n",
    "\n",
    "# Stage 2: Accurate dispatch\n",
    "fs_dispatch = flow_system.transform.fix_sizes(sizes)\n",
    "fs_dispatch.optimize(solver)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You learned how to:\n",
    "\n",
    "- Use **`cluster()`** to reduce time series into typical periods\n",
    "- **Inspect clustering data** with `clustering_data()` before clustering\n",
    "- Use **`data_vars`** to cluster based on specific variables only\n",
    "- Apply **peak forcing** with `ExtremeConfig` to capture extreme demand days\n",
    "- Use **two-stage optimization** for fast yet accurate investment decisions\n",
    "- **Expand solutions** back to full resolution with `expand()`\n",
    "- Access **clustering metadata** via `fs.clustering` (metrics, cluster_assignments, cluster_occurrences)\n",
    "- Use **advanced options** like different algorithms with `ClusterConfig`\n",
    "- **Apply existing clustering** to other FlowSystems using `apply_clustering()`\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Always use peak forcing** (`extremes=ExtremeConfig(max_value=[...])`) for demand time series\n",
    "2. **Inspect data first** with `clustering_data()` to see available variables\n",
    "3. **Use `data_vars`** to cluster on specific variables (e.g., demand only, ignoring prices)\n",
    "4. **Add safety margin** (5-10%) when fixing sizes from clustering\n",
    "5. **Two-stage is recommended**: clustering for sizing, full resolution for dispatch\n",
    "6. **Storage handling** is configurable via `cluster_mode`\n",
    "7. **Check metrics** to evaluate clustering quality\n",
    "8. **Use `apply_clustering()`** to apply the same clustering to different FlowSystem variants\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **[08c2-clustering-storage-modes](08c2-clustering-storage-modes.ipynb)**: Compare storage modes using a seasonal storage system\n",
    "- **[08d-clustering-multiperiod](08d-clustering-multiperiod.ipynb)**: Clustering with multiple periods and scenarios"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
