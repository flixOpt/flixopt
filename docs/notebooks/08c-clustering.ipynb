{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Time Series Clustering with `cluster()`\n",
    "\n",
    "Accelerate investment optimization using typical periods (clustering).\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "- **Typical periods**: Cluster similar time segments (e.g., days) and solve only representative ones\n",
    "- **Weighted costs**: Automatically weight operational costs by cluster occurrence\n",
    "- **Two-stage workflow**: Fast sizing with clustering, accurate dispatch at full resolution\n",
    "\n",
    "!!! note \"Requirements\"\n",
    "    This notebook requires the `tsam` package: `pip install tsam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import flixopt as fx\n",
    "\n",
    "fx.CONFIG.notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Load the FlowSystem\n",
    "\n",
    "We use a pre-built district heating system with real-world time series data (one month at 15-min resolution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.generate_example_systems import create_district_heating_system\n",
    "\n",
    "flow_system = create_district_heating_system()\n",
    "flow_system.connect_and_transform()\n",
    "\n",
    "timesteps = flow_system.timesteps\n",
    "print(f'FlowSystem: {len(timesteps)} timesteps ({len(timesteps) / 96:.0f} days at 15-min resolution)')\n",
    "print(f'Components: {list(flow_system.components.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize input data\n",
    "heat_demand = flow_system.components['HeatDemand'].inputs[0].fixed_relative_profile\n",
    "electricity_price = flow_system.components['GridBuy'].outputs[0].effects_per_flow_hour['costs']\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.1)\n",
    "fig.add_trace(go.Scatter(x=timesteps, y=heat_demand.values, name='Heat Demand', line=dict(width=0.5)), row=1, col=1)\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=timesteps, y=electricity_price.values, name='Electricity Price', line=dict(width=0.5)), row=2, col=1\n",
    ")\n",
    "fig.update_layout(height=400, title='One Month of Input Data')\n",
    "fig.update_yaxes(title_text='Heat Demand [MW]', row=1, col=1)\n",
    "fig.update_yaxes(title_text='El. Price [€/MWh]', row=2, col=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Method 1: Full Optimization (Baseline)\n",
    "\n",
    "First, solve the complete problem with all 2976 timesteps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = fx.solvers.HighsSolver(mip_gap=0.01)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "fs_full = flow_system.copy()\n",
    "fs_full.optimize(solver)\n",
    "time_full = timeit.default_timer() - start\n",
    "\n",
    "print(f'Full optimization: {time_full:.1f} seconds')\n",
    "print(f'Total cost: {fs_full.solution[\"costs\"].item():,.0f} €')\n",
    "print('\\nOptimized sizes:')\n",
    "for name, size in fs_full.statistics.sizes.items():\n",
    "    print(f'  {name}: {float(size.item()):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Method 2: Clustering with `cluster()`\n",
    "\n",
    "The `cluster()` method:\n",
    "\n",
    "1. **Clusters similar days** using the TSAM (Time Series Aggregation Module) package\n",
    "2. **Reduces timesteps** to only typical periods (e.g., 8 typical days = 768 timesteps)\n",
    "3. **Weights costs** by how many original days each typical day represents\n",
    "4. **Handles storage** with configurable behavior via `storage_mode`\n",
    "\n",
    "!!! warning \"Peak Forcing\"\n",
    "    Always use `time_series_for_high_peaks` to ensure extreme demand days are captured.\n",
    "    Without this, clustering may miss peak periods, causing undersized components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "# IMPORTANT: Force inclusion of peak demand periods!\n",
    "peak_series = ['HeatDemand(Q_th)|fixed_relative_profile']\n",
    "\n",
    "# Create reduced FlowSystem with 8 typical days\n",
    "fs_clustered = flow_system.transform.cluster(\n",
    "    n_clusters=8,  # 8 typical days\n",
    "    cluster_duration='1D',  # Daily clustering\n",
    "    time_series_for_high_peaks=peak_series,  # Capture peak demand day\n",
    ")\n",
    "\n",
    "time_clustering = timeit.default_timer() - start\n",
    "print(f'Clustering time: {time_clustering:.1f} seconds')\n",
    "print(f'Reduced: {len(flow_system.timesteps)} → {len(fs_clustered.timesteps)} timesteps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the reduced system\n",
    "start = timeit.default_timer()\n",
    "fs_clustered.optimize(solver)\n",
    "time_clustered = timeit.default_timer() - start\n",
    "\n",
    "print(f'Clustered optimization: {time_clustered:.1f} seconds')\n",
    "print(f'Total cost: {fs_clustered.solution[\"costs\"].item():,.0f} €')\n",
    "print(f'\\nSpeedup vs full: {time_full / (time_clustering + time_clustered):.1f}x')\n",
    "print('\\nOptimized sizes:')\n",
    "for name, size in fs_clustered.statistics.sizes.items():\n",
    "    print(f'  {name}: {float(size.item()):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Understanding the Clustering\n",
    "\n",
    "The clustering algorithm groups similar days together. Let's inspect the cluster structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show clustering info\n",
    "info = fs_clustered.clustering\n",
    "cs = info.result.cluster_structure\n",
    "print('Clustering Configuration:')\n",
    "print(f'  Number of typical periods: {cs.n_clusters}')\n",
    "print(f'  Timesteps per period: {cs.timesteps_per_cluster}')\n",
    "print(f'  Total reduced timesteps: {cs.n_clusters * cs.timesteps_per_cluster}')\n",
    "print(f'  Cluster order (first 10 days): {cs.cluster_order.values[:10]}...')\n",
    "\n",
    "# Show how many times each cluster appears\n",
    "cluster_order = cs.cluster_order.values\n",
    "unique, counts = np.unique(cluster_order, return_counts=True)\n",
    "print('\\nCluster occurrences:')\n",
    "for cluster_id, count in zip(unique, counts, strict=False):\n",
    "    print(f'  Cluster {cluster_id}: {count} days')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Method 3: Two-Stage Workflow (Recommended)\n",
    "\n",
    "The recommended approach for investment optimization:\n",
    "\n",
    "1. **Stage 1**: Fast sizing with `cluster()` \n",
    "2. **Stage 2**: Fix sizes (with safety margin) and dispatch at full resolution\n",
    "\n",
    "!!! tip \"Safety Margin\"\n",
    "    Typical periods aggregate similar days, so individual days may have higher demand \n",
    "    than the typical day. Adding a 5-10% margin ensures feasibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1 already done above\n",
    "print('Stage 1: Sizing with typical periods')\n",
    "print(f'  Time: {time_clustering + time_clustered:.1f} seconds')\n",
    "print(f'  Cost estimate: {fs_clustered.solution[\"costs\"].item():,.0f} €')\n",
    "\n",
    "# Apply safety margin to sizes\n",
    "SAFETY_MARGIN = 1.05  # 5% buffer\n",
    "sizes_with_margin = {name: float(size.item()) * SAFETY_MARGIN for name, size in fs_clustered.statistics.sizes.items()}\n",
    "print(f'\\nSizes with {(SAFETY_MARGIN - 1) * 100:.0f}% safety margin:')\n",
    "for name, size in sizes_with_margin.items():\n",
    "    original = fs_clustered.statistics.sizes[name].item()\n",
    "    print(f'  {name}: {original:.1f} → {size:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Fix sizes and optimize at full resolution\n",
    "print('Stage 2: Dispatch at full resolution')\n",
    "start = timeit.default_timer()\n",
    "\n",
    "fs_dispatch = flow_system.transform.fix_sizes(sizes_with_margin)\n",
    "fs_dispatch.optimize(solver)\n",
    "\n",
    "time_dispatch = timeit.default_timer() - start\n",
    "print(f'  Time: {time_dispatch:.1f} seconds')\n",
    "print(f'  Actual cost: {fs_dispatch.solution[\"costs\"].item():,.0f} €')\n",
    "\n",
    "# Total comparison\n",
    "total_two_stage = time_clustering + time_clustered + time_dispatch\n",
    "print(f'\\nTotal two-stage time: {total_two_stage:.1f} seconds')\n",
    "print(f'Speedup vs full: {time_full / total_two_stage:.1f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'Full (baseline)': {\n",
    "        'Time [s]': time_full,\n",
    "        'Cost [€]': fs_full.solution['costs'].item(),\n",
    "        'CHP': fs_full.statistics.sizes['CHP(Q_th)'].item(),\n",
    "        'Boiler': fs_full.statistics.sizes['Boiler(Q_th)'].item(),\n",
    "        'Storage': fs_full.statistics.sizes['Storage'].item(),\n",
    "    },\n",
    "    'Clustered (8 days)': {\n",
    "        'Time [s]': time_clustering + time_clustered,\n",
    "        'Cost [€]': fs_clustered.solution['costs'].item(),\n",
    "        'CHP': fs_clustered.statistics.sizes['CHP(Q_th)'].item(),\n",
    "        'Boiler': fs_clustered.statistics.sizes['Boiler(Q_th)'].item(),\n",
    "        'Storage': fs_clustered.statistics.sizes['Storage'].item(),\n",
    "    },\n",
    "    'Two-Stage': {\n",
    "        'Time [s]': total_two_stage,\n",
    "        'Cost [€]': fs_dispatch.solution['costs'].item(),\n",
    "        'CHP': sizes_with_margin['CHP(Q_th)'],\n",
    "        'Boiler': sizes_with_margin['Boiler(Q_th)'],\n",
    "        'Storage': sizes_with_margin['Storage'],\n",
    "    },\n",
    "}\n",
    "\n",
    "comparison = pd.DataFrame(results).T\n",
    "baseline_cost = comparison.loc['Full (baseline)', 'Cost [€]']\n",
    "baseline_time = comparison.loc['Full (baseline)', 'Time [s]']\n",
    "comparison['Cost Gap [%]'] = ((comparison['Cost [€]'] - baseline_cost) / abs(baseline_cost) * 100).round(2)\n",
    "comparison['Speedup'] = (baseline_time / comparison['Time [s]']).round(1)\n",
    "\n",
    "comparison.style.format(\n",
    "    {\n",
    "        'Time [s]': '{:.1f}',\n",
    "        'Cost [€]': '{:,.0f}',\n",
    "        'CHP': '{:.1f}',\n",
    "        'Boiler': '{:.1f}',\n",
    "        'Storage': '{:.0f}',\n",
    "        'Cost Gap [%]': '{:.2f}',\n",
    "        'Speedup': '{:.1f}x',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Expand Solution to Full Resolution\n",
    "\n",
    "Use `expand_solution()` to map the clustered solution back to all original timesteps.\n",
    "This repeats the typical period values for all days belonging to that cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand the clustered solution to full resolution\n",
    "fs_expanded = fs_clustered.transform.expand_solution()\n",
    "\n",
    "print(f'Expanded: {len(fs_clustered.timesteps)} → {len(fs_expanded.timesteps)} timesteps')\n",
    "print(f'Cost: {fs_expanded.solution[\"costs\"].item():,.0f} €')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare heat balance: Full vs Expanded\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=['Full Optimization', 'Expanded from Clustering'])\n",
    "\n",
    "# Full\n",
    "for var in ['CHP(Q_th)', 'Boiler(Q_th)']:\n",
    "    values = fs_full.solution[f'{var}|flow_rate'].values\n",
    "    fig.add_trace(go.Scatter(x=fs_full.timesteps, y=values, name=var, legendgroup=var, showlegend=True), row=1, col=1)\n",
    "\n",
    "# Expanded\n",
    "for var in ['CHP(Q_th)', 'Boiler(Q_th)']:\n",
    "    values = fs_expanded.solution[f'{var}|flow_rate'].values\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=fs_expanded.timesteps, y=values, name=var, legendgroup=var, showlegend=False), row=2, col=1\n",
    "    )\n",
    "\n",
    "fig.update_layout(height=500, title='Heat Production Comparison')\n",
    "fig.update_yaxes(title_text='MW', row=1, col=1)\n",
    "fig.update_yaxes(title_text='MW', row=2, col=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Visualize Clustered Heat Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_clustered.statistics.plot.storage('Storage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_expanded.statistics.plot.storage('Storage')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## API Reference\n",
    "\n",
    "### `transform.cluster()` Parameters\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|-----------|------|-------------|\n",
    "| `n_clusters` | `int` | Number of typical periods (e.g., 8 typical days) |\n",
    "| `cluster_duration` | `str \\| float` | Duration per cluster ('1D', '24h') or hours |\n",
    "| `weights` | `dict[str, float]` | Optional weights for time series in clustering |\n",
    "| `time_series_for_high_peaks` | `list[str]` | **Essential**: Force inclusion of peak periods |\n",
    "| `time_series_for_low_peaks` | `list[str]` | Force inclusion of minimum periods |\n",
    "\n",
    "### Storage Behavior\n",
    "\n",
    "Each `Storage` component has a `cluster_storage_mode` parameter that controls how it behaves during clustering:\n",
    "\n",
    "| Mode | Description |\n",
    "|------|-------------|\n",
    "| `'intercluster_cyclic'` | Links storage across clusters + yearly cyclic **(default)** |\n",
    "| `'intercluster'` | Links storage across clusters, free start/end |\n",
    "| `'cyclic'` | Each cluster is independent but cyclic (start = end) |\n",
    "| `'independent'` | Each cluster is independent, free start/end |\n",
    "\n",
    "For a detailed comparison of storage modes, see [08c2-clustering-storage-modes](08c2-clustering-storage-modes.ipynb).\n",
    "\n",
    "### Peak Forcing Format\n",
    "\n",
    "```python\n",
    "time_series_for_high_peaks = ['ComponentName(FlowName)|fixed_relative_profile']\n",
    "```\n",
    "\n",
    "### Recommended Workflow\n",
    "\n",
    "```python\n",
    "# Stage 1: Fast sizing\n",
    "fs_sizing = flow_system.transform.cluster(\n",
    "    n_clusters=8,\n",
    "    cluster_duration='1D',\n",
    "    time_series_for_high_peaks=['Demand(Flow)|fixed_relative_profile'],\n",
    ")\n",
    "fs_sizing.optimize(solver)\n",
    "\n",
    "# Apply safety margin\n",
    "sizes = {k: v.item() * 1.05 for k, v in fs_sizing.statistics.sizes.items()}\n",
    "\n",
    "# Stage 2: Accurate dispatch\n",
    "fs_dispatch = flow_system.transform.fix_sizes(sizes)\n",
    "fs_dispatch.optimize(solver)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You learned how to:\n",
    "\n",
    "- Use **`cluster()`** to reduce time series into typical periods\n",
    "- Apply **peak forcing** to capture extreme demand days\n",
    "- Use **two-stage optimization** for fast yet accurate investment decisions\n",
    "- **Expand solutions** back to full resolution with `expand_solution()`\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Always use peak forcing** (`time_series_for_high_peaks`) for demand time series\n",
    "2. **Add safety margin** (5-10%) when fixing sizes from clustering\n",
    "3. **Two-stage is recommended**: clustering for sizing, full resolution for dispatch\n",
    "4. **Storage handling** is configurable via `storage_mode`\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **[08c2-clustering-storage-modes](08c2-clustering-storage-modes.ipynb)**: Compare storage modes using a seasonal storage system\n",
    "- **[08d-clustering-multiperiod](08d-clustering-multiperiod.ipynb)**: Clustering with multiple periods and scenarios"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
