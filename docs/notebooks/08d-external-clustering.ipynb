{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typical Periods Optimization with `cluster_reduce()`\n",
    "\n",
    "This notebook demonstrates the new `cluster_reduce()` method for fast sizing optimization using typical periods.\n",
    "\n",
    "## Key Concept\n",
    "\n",
    "Unlike `cluster()` which uses equality constraints (same number of timesteps), `cluster_reduce()` **actually reduces** the number of timesteps:\n",
    "\n",
    "| Method | Timesteps | Mechanism | Use Case |\n",
    "|--------|-----------|-----------|----------|\n",
    "| `cluster()` | 8760 | Equality constraints | Accurate operational dispatch |\n",
    "| `cluster_reduce()` | 192 (8×24) | Typical periods only | Fast initial sizing |\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Actual timestep reduction**: Only solves for typical periods (e.g., 8 days × 24h = 192 instead of 8760)\n",
    "- **Timestep weighting**: Operational costs are weighted by cluster occurrence\n",
    "- **Inter-period storage linking**: SOC_boundary variables track storage state across original periods\n",
    "- **Cyclic constraint**: Optional cyclic storage constraint for long-term balance\n",
    "\n",
    "!!! note \"Requirements\"\n",
    "    This notebook requires the `tsam` package: `pip install tsam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import flixopt as fx\n",
    "\n",
    "fx.CONFIG.notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Full-Year Example System\n",
    "\n",
    "We'll create a simple district heating system with a full year of hourly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic yearly data\n",
    "np.random.seed(42)\n",
    "hours = 8760  # Full year hourly\n",
    "\n",
    "# Create realistic heat demand profile (seasonal + daily patterns)\n",
    "t = np.arange(hours)\n",
    "seasonal = 50 + 40 * np.cos(2 * np.pi * t / 8760)  # Higher in winter\n",
    "daily = 10 * np.sin(2 * np.pi * t / 24 - np.pi / 2)  # Peak in morning/evening\n",
    "noise = np.random.normal(0, 5, hours)\n",
    "heat_demand = np.maximum(seasonal + daily + noise, 10)\n",
    "\n",
    "# Create electricity price profile (higher during day, lower at night)\n",
    "hour_of_day = t % 24\n",
    "elec_price = 50 + 30 * np.sin(np.pi * hour_of_day / 12) + np.random.normal(0, 5, hours)\n",
    "elec_price = np.maximum(elec_price, 20)\n",
    "\n",
    "timesteps = pd.date_range('2020-01-01', periods=hours, freq='h')\n",
    "\n",
    "print(f'Created {hours} hourly timesteps ({hours / 24:.0f} days)')\n",
    "print(f'Heat demand range: {heat_demand.min():.1f} - {heat_demand.max():.1f} MW')\n",
    "print(f'Electricity price range: {elec_price.min():.1f} - {elec_price.max():.1f} EUR/MWh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first month of data\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=timesteps[:720], y=heat_demand[:720], name='Heat Demand'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=timesteps[:720], y=elec_price[:720], name='Electricity Price'), row=2, col=1)\n",
    "\n",
    "fig.update_layout(height=400, title='First Month of Data')\n",
    "fig.update_yaxes(title_text='Heat Demand [MW]', row=1, col=1)\n",
    "fig.update_yaxes(title_text='El. Price [EUR/MWh]', row=2, col=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flow_system():\n",
    "    \"\"\"Create the district heating FlowSystem.\"\"\"\n",
    "    fs = fx.FlowSystem(timesteps=timesteps)\n",
    "\n",
    "    # Effects\n",
    "    costs = fx.Effect(label='costs', unit='EUR', is_objective=True)\n",
    "\n",
    "    # Buses\n",
    "    heat_bus = fx.Bus('Heat')\n",
    "    elec_bus = fx.Bus('Electricity')\n",
    "    gas_bus = fx.Bus('Gas')\n",
    "\n",
    "    fs.add_elements(costs, heat_bus, elec_bus, gas_bus)\n",
    "\n",
    "    # Gas supply\n",
    "    gas_supply = fx.Source(\n",
    "        'GasSupply',\n",
    "        outputs=[fx.Flow('gas_out', bus='Gas', size=500, effects_per_flow_hour={'costs': 35})],\n",
    "    )\n",
    "\n",
    "    # Electricity grid\n",
    "    grid_buy = fx.Source(\n",
    "        'GridBuy',\n",
    "        outputs=[fx.Flow('elec_out', bus='Electricity', size=200, effects_per_flow_hour={'costs': elec_price})],\n",
    "    )\n",
    "\n",
    "    grid_sell = fx.Sink(\n",
    "        'GridSell',\n",
    "        inputs=[fx.Flow('elec_in', bus='Electricity', size=200, effects_per_flow_hour={'costs': -elec_price * 0.9})],\n",
    "    )\n",
    "\n",
    "    # Boiler (investment)\n",
    "    boiler = fx.linear_converters.Boiler(\n",
    "        'Boiler',\n",
    "        thermal_efficiency=0.9,\n",
    "        thermal_flow=fx.Flow(\n",
    "            'Q_th',\n",
    "            bus='Heat',\n",
    "            size=fx.InvestParameters(minimum_size=0, maximum_size=200, effects_of_investment_per_size={'costs': 50000}),\n",
    "        ),\n",
    "        fuel_flow=fx.Flow('Q_fu', bus='Gas'),\n",
    "    )\n",
    "\n",
    "    # CHP (investment)\n",
    "    chp = fx.linear_converters.CHP(\n",
    "        'CHP',\n",
    "        thermal_efficiency=0.45,\n",
    "        electrical_efficiency=0.35,\n",
    "        thermal_flow=fx.Flow(\n",
    "            'Q_th',\n",
    "            bus='Heat',\n",
    "            size=fx.InvestParameters(\n",
    "                minimum_size=0, maximum_size=150, effects_of_investment_per_size={'costs': 150000}\n",
    "            ),\n",
    "        ),\n",
    "        electrical_flow=fx.Flow('P_el', bus='Electricity'),\n",
    "        fuel_flow=fx.Flow('Q_fu', bus='Gas'),\n",
    "    )\n",
    "\n",
    "    # Heat storage (investment)\n",
    "    storage = fx.Storage(\n",
    "        'ThermalStorage',\n",
    "        charging=fx.Flow('charge', bus='Heat', size=50),\n",
    "        discharging=fx.Flow('discharge', bus='Heat', size=50),\n",
    "        capacity_in_flow_hours=fx.InvestParameters(\n",
    "            minimum_size=0, maximum_size=500, effects_of_investment_per_size={'costs': 20000}\n",
    "        ),\n",
    "        eta_charge=0.95,\n",
    "        eta_discharge=0.95,\n",
    "        relative_loss_per_hour=0.005,\n",
    "        initial_charge_state='equals_final',\n",
    "    )\n",
    "\n",
    "    # Heat demand\n",
    "    demand = fx.Sink(\n",
    "        'HeatDemand',\n",
    "        inputs=[fx.Flow('Q_th', bus='Heat', size=1, fixed_relative_profile=heat_demand)],\n",
    "    )\n",
    "\n",
    "    fs.add_elements(gas_supply, grid_buy, grid_sell, boiler, chp, storage, demand)\n",
    "\n",
    "    return fs\n",
    "\n",
    "\n",
    "# Create the system\n",
    "flow_system = create_flow_system()\n",
    "print(f'FlowSystem created with {len(flow_system.timesteps)} timesteps')\n",
    "print(f'Components: {list(flow_system.components.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Full Optimization (Baseline)\n",
    "\n",
    "First, let's solve the full problem with all 8760 timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "solver = fx.solvers.HighsSolver(mip_gap=0.01)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "fs_full = create_flow_system()\n",
    "fs_full.optimize(solver)\n",
    "time_full = timeit.default_timer() - start\n",
    "\n",
    "print(f'Full optimization: {time_full:.2f} seconds')\n",
    "print(f'Total cost: {fs_full.solution[\"costs\"].item():,.0f} EUR')\n",
    "print('\\nOptimized sizes:')\n",
    "for name, size in fs_full.statistics.sizes.items():\n",
    "    print(f'  {name}: {float(size.item()):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Typical Periods with `cluster_reduce()`\n",
    "\n",
    "Now let's use the new `cluster_reduce()` method to solve with only 8 typical days (192 timesteps).\n",
    "\n",
    "**Important**: Use `time_series_for_high_peaks` to force inclusion of peak demand periods. Without this, the typical periods may miss extreme peaks, leading to undersized components that cause infeasibility in the full-resolution dispatch stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "# IMPORTANT: Use time_series_for_high_peaks to force inclusion of peak demand periods!\n",
    "# Without this, the typical periods may miss extreme peaks, leading to undersized components.\n",
    "# The format is the column name in the internal dataframe: 'ComponentName(FlowName)|attribute'\n",
    "peak_forcing_series = ['HeatDemand(Q_th)|fixed_relative_profile']\n",
    "\n",
    "# Create reduced FlowSystem with 8 typical days\n",
    "fs_reduced = create_flow_system().transform.cluster_reduce(\n",
    "    hours_per_period=24,  # 24 hours per period (daily)\n",
    "    nr_of_typical_periods=8,  # 8 typical days\n",
    "    time_series_for_high_peaks=peak_forcing_series,  # Force inclusion of peak demand day!\n",
    "    storage_inter_period_linking=True,  # Link storage states between periods\n",
    "    storage_cyclic=True,  # Cyclic constraint: SOC[0] = SOC[end]\n",
    ")\n",
    "\n",
    "time_clustering = timeit.default_timer() - start\n",
    "print(f'Clustering time: {time_clustering:.2f} seconds')\n",
    "print(f'Reduced from {len(flow_system.timesteps)} to {len(fs_reduced.timesteps)} timesteps')\n",
    "print(f'Timestep weights (cluster occurrences): {np.unique(fs_reduced._typical_periods_info[\"timestep_weights\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the reduced system\n",
    "start = timeit.default_timer()\n",
    "fs_reduced.optimize(solver)\n",
    "time_reduced = timeit.default_timer() - start\n",
    "\n",
    "print(f'Reduced optimization: {time_reduced:.2f} seconds')\n",
    "print(f'Total cost: {fs_reduced.solution[\"costs\"].item():,.0f} EUR')\n",
    "print(f'Speedup vs full: {time_full / (time_clustering + time_reduced):.1f}x')\n",
    "print('\\nOptimized sizes:')\n",
    "for name, size in fs_reduced.statistics.sizes.items():\n",
    "    print(f'  {name}: {float(size.item()):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Two-Stage Workflow\n",
    "\n",
    "The recommended workflow:\n",
    "1. **Stage 1**: Fast sizing with `cluster_reduce()`\n",
    "2. **Stage 2**: Fix sizes (with safety margin) and re-optimize for accurate dispatch\n",
    "\n",
    "**Note**: Typical periods aggregate similar days, so individual days within a cluster may have higher demand than the typical day. Adding a 5-10% safety margin to sizes helps ensure feasibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Fast sizing (already done above)\n",
    "print('Stage 1: Sizing with typical periods')\n",
    "print(f'  Time: {time_clustering + time_reduced:.2f} seconds')\n",
    "print(f'  Cost estimate: {fs_reduced.solution[\"costs\"].item():,.0f} EUR')\n",
    "\n",
    "# Apply safety margin to sizes (5-10% buffer for demand variability)\n",
    "SAFETY_MARGIN = 1.05  # 5% buffer\n",
    "sizes_with_margin = {name: float(size.item()) * SAFETY_MARGIN for name, size in fs_reduced.statistics.sizes.items()}\n",
    "print(f'\\nSizes with {(SAFETY_MARGIN - 1) * 100:.0f}% safety margin:')\n",
    "for name, size in sizes_with_margin.items():\n",
    "    original = fs_reduced.statistics.sizes[name].item()\n",
    "    print(f'  {name}: {original:.1f} -> {size:.1f}')\n",
    "\n",
    "# Stage 2: Fix sizes and re-optimize at full resolution\n",
    "print('\\nStage 2: Dispatch at full resolution')\n",
    "start = timeit.default_timer()\n",
    "\n",
    "fs_dispatch = create_flow_system().transform.fix_sizes(sizes_with_margin)\n",
    "fs_dispatch.optimize(solver)\n",
    "\n",
    "time_dispatch = timeit.default_timer() - start\n",
    "print(f'  Time: {time_dispatch:.2f} seconds')\n",
    "print(f'  Actual cost: {fs_dispatch.solution[\"costs\"].item():,.0f} EUR')\n",
    "\n",
    "# Total time comparison\n",
    "total_two_stage = time_clustering + time_reduced + time_dispatch\n",
    "print(f'\\nTotal two-stage time: {total_two_stage:.2f} seconds')\n",
    "print(f'Full optimization time: {time_full:.2f} seconds')\n",
    "print(f'Two-stage speedup: {time_full / total_two_stage:.1f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'Full (baseline)': {\n",
    "        'Time [s]': time_full,\n",
    "        'Cost [EUR]': fs_full.solution['costs'].item(),\n",
    "        'Boiler Size': fs_full.statistics.sizes['Boiler(Q_th)'].item(),\n",
    "        'CHP Size': fs_full.statistics.sizes['CHP(Q_th)'].item(),\n",
    "        'Storage Size': fs_full.statistics.sizes['ThermalStorage'].item(),\n",
    "    },\n",
    "    'Typical Periods (sizing)': {\n",
    "        'Time [s]': time_clustering + time_reduced,\n",
    "        'Cost [EUR]': fs_reduced.solution['costs'].item(),\n",
    "        'Boiler Size': fs_reduced.statistics.sizes['Boiler(Q_th)'].item(),\n",
    "        'CHP Size': fs_reduced.statistics.sizes['CHP(Q_th)'].item(),\n",
    "        'Storage Size': fs_reduced.statistics.sizes['ThermalStorage'].item(),\n",
    "    },\n",
    "    'Two-Stage (with margin)': {\n",
    "        'Time [s]': total_two_stage,\n",
    "        'Cost [EUR]': fs_dispatch.solution['costs'].item(),\n",
    "        'Boiler Size': sizes_with_margin['Boiler(Q_th)'],\n",
    "        'CHP Size': sizes_with_margin['CHP(Q_th)'],\n",
    "        'Storage Size': sizes_with_margin['ThermalStorage'],\n",
    "    },\n",
    "}\n",
    "\n",
    "comparison = pd.DataFrame(results).T\n",
    "baseline_cost = comparison.loc['Full (baseline)', 'Cost [EUR]']\n",
    "baseline_time = comparison.loc['Full (baseline)', 'Time [s]']\n",
    "comparison['Cost Gap [%]'] = ((comparison['Cost [EUR]'] - baseline_cost) / abs(baseline_cost) * 100).round(2)\n",
    "comparison['Speedup'] = (baseline_time / comparison['Time [s]']).round(1)\n",
    "\n",
    "comparison.style.format(\n",
    "    {\n",
    "        'Time [s]': '{:.2f}',\n",
    "        'Cost [EUR]': '{:,.0f}',\n",
    "        'Boiler Size': '{:.1f}',\n",
    "        'CHP Size': '{:.1f}',\n",
    "        'Storage Size': '{:.0f}',\n",
    "        'Cost Gap [%]': '{:.2f}',\n",
    "        'Speedup': '{:.1f}x',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-Period Storage Linking\n",
    "\n",
    "The `cluster_reduce()` method creates special constraints to track storage state across original periods:\n",
    "\n",
    "- **SOC_boundary[d]**: Storage state at the boundary of original period d\n",
    "- **delta_SOC[c]**: Change in SOC during typical period c\n",
    "- **Linking**: `SOC_boundary[d+1] = SOC_boundary[d] + delta_SOC[cluster_order[d]]`\n",
    "- **Cyclic**: `SOC_boundary[0] = SOC_boundary[end]` (optional)\n",
    "\n",
    "This ensures long-term storage behavior is captured correctly even though we only solve for typical periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show clustering info\n",
    "info = fs_reduced._typical_periods_info\n",
    "print('Typical Periods Configuration:')\n",
    "print(f'  Number of typical periods: {info[\"nr_of_typical_periods\"]}')\n",
    "print(f'  Timesteps per period: {info[\"timesteps_per_period\"]}')\n",
    "print(f'  Total reduced timesteps: {info[\"nr_of_typical_periods\"] * info[\"timesteps_per_period\"]}')\n",
    "print(f'  Cluster order (first 10): {info[\"cluster_order\"][:10]}...')\n",
    "print(f'  Cluster occurrences: {dict(info[\"cluster_occurrences\"])}')\n",
    "print(f'  Storage inter-period linking: {info[\"storage_inter_period_linking\"]}')\n",
    "print(f'  Storage cyclic: {info[\"storage_cyclic\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Reference\n",
    "\n",
    "### `transform.cluster_reduce()` Parameters\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|-----------|------|-------------|\n",
    "| `hours_per_period` | `float` | Duration of each period in hours (e.g., 24 for daily) |\n",
    "| `nr_of_typical_periods` | `int` | Number of typical periods to extract (e.g., 8) |\n",
    "| `weights` | `dict[str, float]` | Optional weights for clustering each time series |\n",
    "| `time_series_for_high_peaks` | `list[str]` | **IMPORTANT**: Force inclusion of high-value periods to capture peak demands |\n",
    "| `time_series_for_low_peaks` | `list[str]` | Force inclusion of low-value periods |\n",
    "| `storage_inter_period_linking` | `bool` | Link storage states between periods (default: True) |\n",
    "| `storage_cyclic` | `bool` | Enforce cyclic storage constraint (default: True) |\n",
    "\n",
    "### Peak Forcing\n",
    "\n",
    "**Always use `time_series_for_high_peaks`** for demand time series to ensure extreme peaks are captured. The format is:\n",
    "```python\n",
    "time_series_for_high_peaks=['ComponentName(FlowName)|fixed_relative_profile']\n",
    "```\n",
    "\n",
    "Without peak forcing, the clustering algorithm may select typical periods that don't include the peak demand day, leading to undersized components and infeasibility in the dispatch stage.\n",
    "\n",
    "### Comparison with `cluster()`\n",
    "\n",
    "| Feature | `cluster()` | `cluster_reduce()` |\n",
    "|---------|-------------|--------------------|\n",
    "| Timesteps | Original (8760) | Reduced (e.g., 192) |\n",
    "| Mechanism | Equality constraints | Typical periods only |\n",
    "| Solve time | Moderate reduction | Dramatic reduction |\n",
    "| Accuracy | Higher | Lower (sizing only) |\n",
    "| Storage handling | Via constraints | SOC boundary linking |\n",
    "| Use case | Final dispatch | Initial sizing |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The new `cluster_reduce()` method provides:\n",
    "\n",
    "1. **Dramatic speedup** for sizing optimization by reducing timesteps\n",
    "2. **Proper cost weighting** so operational costs reflect cluster occurrences\n",
    "3. **Storage state tracking** across original periods via SOC_boundary variables\n",
    "4. **Two-stage workflow** support via `fix_sizes()` for accurate dispatch\n",
    "\n",
    "### Recommended Workflow\n",
    "\n",
    "```python\n",
    "# Stage 1: Fast sizing with typical periods\n",
    "fs_sizing = flow_system.transform.cluster_reduce(\n",
    "    hours_per_period=24,\n",
    "    nr_of_typical_periods=8,\n",
    "    time_series_for_high_peaks=['DemandComponent(FlowName)|fixed_relative_profile'],\n",
    ")\n",
    "fs_sizing.optimize(solver)\n",
    "\n",
    "# Apply safety margin (typical periods aggregate, so individual days may exceed)\n",
    "SAFETY_MARGIN = 1.05  # 5% buffer\n",
    "sizes_with_margin = {\n",
    "    name: float(size.item()) * SAFETY_MARGIN\n",
    "    for name, size in fs_sizing.statistics.sizes.items()\n",
    "}\n",
    "\n",
    "# Stage 2: Fix sizes and optimize dispatch at full resolution\n",
    "fs_dispatch = flow_system.transform.fix_sizes(sizes_with_margin)\n",
    "fs_dispatch.optimize(solver)\n",
    "```\n",
    "\n",
    "### Key Considerations\n",
    "\n",
    "- **Peak forcing is essential**: Use `time_series_for_high_peaks` to capture peak demand days\n",
    "- **Safety margin recommended**: Add 5-10% buffer to sizes since aggregation smooths peaks\n",
    "- **Two-stage is recommended**: Use `cluster_reduce()` for fast sizing, then `fix_sizes()` for dispatch\n",
    "- **Storage linking preserves long-term behavior**: SOC_boundary variables ensure correct storage cycling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
